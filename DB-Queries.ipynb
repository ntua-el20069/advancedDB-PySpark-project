{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca4a005-f7c2-4eae-9f3f-07039e4e93b0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b70915-8adb-4c82-8250-5292395b78b6",
   "metadata": {},
   "source": [
    "## Execution Instructions\n",
    "\n",
    "- execute the `Time Measurement` cell in order to use the `@measure_time` decorator\n",
    "- each query can be executed independently from other queries\n",
    "- For Query 2, ensure `s3_path` points to a directory where the parquet file can be stored (else change the path or create dir)\n",
    "- For Queries 4, 5\n",
    "    1. select the configuration for `spark.executor` by executing the corresponding cell from `Configurations`\n",
    "    2. execute the `Time Measurement` cell\n",
    "    3. execute corresponding `PySpark & Sedona imports, Read Datasets, register functions` cells\n",
    "    4. execute the cell defining the query execution function\n",
    "    5. run a cell from `Experiments` section\n",
    "    6. repeat for each configuration to be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1a6c5-0af5-4465-844a-989baa7c91ce",
   "metadata": {},
   "source": [
    "## Time Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec3ea8e-47b3-4b6c-97ff-466be272bb66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3313</td><td>application_1732639283265_3269</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3269/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3269_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "# decorator to calculate duration\n",
    "# taken by any function.\n",
    "def measure_time(func):\n",
    "    # added arguments inside the inner1,\n",
    "    # if function takes any arguments,\n",
    "    # can be added like this.\n",
    "    def inner1(*args, **kwargs):\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        func(*args, **kwargs)\n",
    "\n",
    "        # Stop timing and print out the execution duration\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Time taken by {func.__name__}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return inner1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b41a1-3cbc-4173-87ea-e626bd2789d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74616b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3505</td><td>application_1732639283265_3461</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3461/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3461_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d59a115-6126-46fe-af39-6a95adb15a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crime_data_19 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_data_20 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_data = crime_data_19.union(crime_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567e5a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|   Age_Group| count|\n",
      "+------------+------+\n",
      "|      Adults|121093|\n",
      "|Young Adults| 33605|\n",
      "|    Children| 15928|\n",
      "|     Seniors|  5985|\n",
      "+------------+------+\n",
      "\n",
      "DataFrame API execution time:  5.511902332305908"
     ]
    }
   ],
   "source": [
    "## DataFrame APIs ##\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "import time\n",
    "\n",
    "start_time_df = time.time()\n",
    "filtered_data = crime_data.filter(col(\"Crm Cd Desc\").like(\"%AGGRAVATED ASSAULT%\"))\n",
    "updated_df = filtered_data.withColumn(\n",
    "    \"Age_Group\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") < 25), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") < 65), \"Adults\")\n",
    "    .otherwise(\"Seniors\")\n",
    ")\n",
    "\n",
    "age_group_counts = updated_df.groupBy(\"Age_Group\").count()\n",
    "age_group_counts = age_group_counts.orderBy(col(\"count\").desc())\n",
    "age_group_counts.show()\n",
    "end_time_df = time.time()\n",
    "print(\"DataFrame API execution time: \", end_time_df - start_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "474a04ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults -> 121093\n",
      "Young Adults -> 33605\n",
      "Children -> 15928\n",
      "Seniors -> 5985\n",
      "RDD API execution time:  22.607872486114502"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "import time \n",
    "\n",
    "start_time_rdd = time.time()\n",
    "\n",
    "crime_rdd = crime_data.rdd\n",
    "filtered_rdd = crime_rdd.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[\"Crm Cd Desc\"])\n",
    "\n",
    "def get_age_group(age):\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif 18 <= age < 25:\n",
    "        return \"Young Adults\"\n",
    "    elif 25 <= age < 65:\n",
    "        return \"Adults\"\n",
    "    else:\n",
    "        return \"Seniors\"\n",
    "\n",
    "age_group_rdd = filtered_rdd.map(lambda x: (get_age_group(x['Vict Age']), 1))\n",
    "age_group_count = age_group_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "sorted_age_group_count = age_group_count.sortBy(lambda x: x[1], ascending=False)\n",
    "for age_group, count in sorted_age_group_count.collect():\n",
    "    print(age_group, \"->\", count)\n",
    "end_time_rdd = time.time()\n",
    "print(\"RDD API execution time: \", end_time_rdd - start_time_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190026a-3ca5-45e6-a21c-96da98373e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c830f3ad-77c5-4c2d-bc19-07233ce77b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, BooleanType\n",
    "from pyspark.sql.functions import col, udf, sum, max, min, avg, count, mean, when, monotonically_increasing_id, dense_rank, window\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2: 3 Police Stations for each year with biggest rate of closed cases\") \\\n",
    "    .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a9bcf68-b586-42fe-bb5f-4b50a4355921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DR_NO: string (nullable = true)\n",
      " |-- Date Rptd: string (nullable = true)\n",
      " |-- DATE OCC: string (nullable = true)\n",
      " |-- TIME OCC: string (nullable = true)\n",
      " |-- AREA : string (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: string (nullable = true)\n",
      " |-- Part 1-2: string (nullable = true)\n",
      " |-- Crm Cd: string (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: string (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: string (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: string (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: string (nullable = true)\n",
      " |-- Crm Cd 2: string (nullable = true)\n",
      " |-- Crm Cd 3: string (nullable = true)\n",
      " |-- Crm Cd 4: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: string (nullable = true)\n",
      " |-- LON: string (nullable = true)\n",
      "\n",
      "Number of Rows (Crime DataFrame)\n",
      "3113337"
     ]
    }
   ],
   "source": [
    "crimes_2010_19_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "crimes_2020_24_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True)\n",
    "crimes_df = crimes_2010_19_df.union(crimes_2020_24_df)\n",
    "crimes_df.printSchema()\n",
    "print(\"Number of Rows (Crime DataFrame)\")\n",
    "crimes_df.count()\n",
    "# print('Crime data')\n",
    "# crimes_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1954855-4159-43ad-8678-9d707e91907e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function percentage at 0x7f35087f01f0>"
     ]
    }
   ],
   "source": [
    "# UDF - User Defined Functions definitions\n",
    "\n",
    "def extract_year(date_occ: str) -> str:\n",
    "    '''returns year from DATE OCC column'''\n",
    "    return date_occ.split(\"/\")[2].split(\" \")[0]\n",
    "\n",
    "def is_closed_case(case: str) -> int:\n",
    "    '''returns 1 if an incident is a closed case in police department based on Status Desc else returns 0'''\n",
    "    return 0 if (case=='Invest Cont' or case=='UNK') else 1\n",
    "\n",
    "def percentage(closed: int, total: int) -> float:\n",
    "    return (closed/total)*100\n",
    "\n",
    "# print(is_closed_case('c'))\n",
    "# print(extract_year(\"01/01/2010 12:00:...\"))\n",
    "extract_year_udf = udf(extract_year, StringType())\n",
    "is_closed_case_udf = udf(is_closed_case, IntegerType())\n",
    "percentage_udf = udf(percentage, FloatType())\n",
    "\n",
    "# register functions for SQL\n",
    "spark.udf.register(\"extract_year\", extract_year)\n",
    "spark.udf.register(\"is_closed_case\", is_closed_case)\n",
    "spark.udf.register(\"percentage\", percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c1ecd5-c501-455e-9a2f-02e19d16e057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# s3://groups-bucket-dblab-905418150721/group46/query2/\n",
    "# s3://groups-bucket-dblab-905418150721/group46/query2-single-parquet/\n",
    "# create the output directory in your S3 bucket\n",
    "group_number = \"46\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/query2-single-parquet/\"\n",
    "s3_path_multiple_parquet = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/query2/\"\n",
    "\n",
    "# Repartition the DataFrame to a single partition so that it will possible be written in one parquet file\n",
    "single_partition_df = crimes_df.repartition(1) \n",
    "single_partition_df.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "# else write parquet simply - it may be stored in multiple files\n",
    "crimes_df.write.mode(\"overwrite\").parquet(s3_path_multiple_parquet)\n",
    "\n",
    "# crimes_df_from_parquet = spark.read.parquet(s3_path)\n",
    "# crimes_df_from_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242faca5-3d5b-48ee-981e-f3bc2024d10a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query 2 - DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe5bf81b-a2f8-402f-b98c-53d3e103f6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@measure_time\n",
    "def query2_dataframe(df, debug = False):\n",
    "    '''Returns the DF that is result of the query 2 using DataFrame API'''\n",
    "\n",
    "    modified_df = df.select(\"DR_NO\", \"DATE OCC\",\"AREA NAME\", \"Status Desc\") \\\n",
    "        .withColumn(\"year\", extract_year_udf(col(\"DATE OCC\"))) \\\n",
    "        .withColumn(\"precinct\", col(\"AREA NAME\")) \\\n",
    "        .withColumn(\"is_closed_case\", is_closed_case_udf(col(\"Status Desc\")))\n",
    "    if debug: modified_df.show(3)\n",
    "\n",
    "    grouped_df = modified_df.groupBy(\"year\", \"precinct\") \\\n",
    "        .agg( \\\n",
    "             count(\"*\").alias(\"total_cases\"), \\\n",
    "             sum(\"is_closed_case\").alias(\"closed_cases\"), \\\n",
    "             percentage_udf(col(\"closed_cases\"), col(\"total_cases\")).alias(\"closed_case_rate\") \\\n",
    "            )\n",
    "    if debug: grouped_df.show(3)\n",
    "\n",
    "    # Define a window and make partitions by year in order to assign specific rank values later to the rows\n",
    "    windowSpec = Window.partitionBy(\"year\").orderBy(col(\"closed_case_rate\").desc())\n",
    "    ranked_df = grouped_df.withColumn(\"#\", dense_rank().over(windowSpec))\n",
    "    if debug: ranked_df.show(3)\n",
    "\n",
    "    # Project specific columns, Select Top 3 for each year and sort in ascending order for year and rank (#)\n",
    "    result = ranked_df.select(\"year\", \"precinct\",\"closed_case_rate\", \"#\") \\\n",
    "        .filter(col(\"#\") <= 3) \\\n",
    "        .orderBy([\"year\", \"#\"], ascending=[True,True])\n",
    "\n",
    "    result.show(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc493a-3357-4ecc-bf86-71bcd816ea63",
   "metadata": {},
   "source": [
    "### Comparison of query execution time with input from CSV or Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db4fc25-c385-42f7-9c4d-1f02f77be346",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@measure_time\n",
    "def query2_csv(execute_query = True):\n",
    "    crimes_2010_19_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "    crimes_2020_24_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True)\n",
    "    crimes_df = crimes_2010_19_df.union(crimes_2020_24_df)\n",
    "    # crimes_df.show(3)\n",
    "    if execute_query: query2_dataframe(crimes_df)\n",
    "\n",
    "@measure_time\n",
    "def query2_parquet(path: str, execute_query = True):\n",
    "    crimes_df_from_parquet = spark.read.parquet(path)\n",
    "    # crimes_df_from_parquet.show(3)\n",
    "    if execute_query: query2_dataframe(crimes_df_from_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b06c6ae-d734-4a37-bd31-7b3713549de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+---+\n",
      "|year|   precinct|closed_case_rate|  #|\n",
      "+----+-----------+----------------+---+\n",
      "|2010|    Rampart|       32.847134|  1|\n",
      "|2010|    Olympic|        31.51529|  2|\n",
      "|2010|     Harbor|       29.360283|  3|\n",
      "|2011|    Olympic|       35.040062|  1|\n",
      "|2011|    Rampart|        32.49645|  2|\n",
      "|2011|     Harbor|       28.513363|  3|\n",
      "|2012|    Olympic|       34.297085|  1|\n",
      "|2012|    Rampart|       32.460003|  2|\n",
      "|2012|     Harbor|       29.509586|  3|\n",
      "|2013|    Olympic|        33.58218|  1|\n",
      "|2013|    Rampart|       32.106037|  2|\n",
      "|2013|     Harbor|       29.723639|  3|\n",
      "|2014|   Van Nuys|       32.021523|  1|\n",
      "|2014|West Valley|       31.497547|  2|\n",
      "|2014|    Mission|        31.22494|  3|\n",
      "|2015|   Van Nuys|        32.26514|  1|\n",
      "|2015|    Mission|       30.463762|  2|\n",
      "|2015|   Foothill|       30.353003|  3|\n",
      "|2016|   Van Nuys|        32.19452|  1|\n",
      "|2016|West Valley|       31.401464|  2|\n",
      "|2016|   Foothill|       29.908648|  3|\n",
      "|2017|   Van Nuys|       32.055428|  1|\n",
      "|2017|    Mission|       31.055387|  2|\n",
      "|2017|   Foothill|         30.4697|  3|\n",
      "|2018|   Foothill|       30.731346|  1|\n",
      "|2018|    Mission|       30.727024|  2|\n",
      "|2018|   Van Nuys|       28.905207|  3|\n",
      "|2019|    Mission|       30.727411|  1|\n",
      "|2019|West Valley|       30.579742|  2|\n",
      "|2019|N Hollywood|       29.238087|  3|\n",
      "|2020|West Valley|       30.771132|  1|\n",
      "|2020|    Mission|       30.149746|  2|\n",
      "|2020|     Harbor|       29.693487|  3|\n",
      "|2021|    Mission|       30.318115|  1|\n",
      "|2021|West Valley|       28.971087|  2|\n",
      "|2021|   Foothill|       27.993757|  3|\n",
      "|2022|West Valley|       26.536367|  1|\n",
      "|2022|     Harbor|       26.337538|  2|\n",
      "|2022|    Topanga|       26.234013|  3|\n",
      "|2023|   Foothill|        26.76076|  1|\n",
      "|2023|    Topanga|       26.538023|  2|\n",
      "|2023|    Mission|       25.662731|  3|\n",
      "|2024|N Hollywood|        19.59853|  1|\n",
      "|2024|   Foothill|       18.620882|  2|\n",
      "|2024|77th Street|       17.586319|  3|\n",
      "+----+-----------+----------------+---+\n",
      "\n",
      "Time taken by query2_dataframe: 5.96 seconds\n",
      "Time taken by query2_csv: 6.63 seconds"
     ]
    }
   ],
   "source": [
    "# Call the function that loads DF from CSV and then executes the query \n",
    "query2_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea578e56-8f53-40ed-8e14-a7abdd7378e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+---+\n",
      "|year|   precinct|closed_case_rate|  #|\n",
      "+----+-----------+----------------+---+\n",
      "|2010|    Rampart|       32.847134|  1|\n",
      "|2010|    Olympic|        31.51529|  2|\n",
      "|2010|     Harbor|       29.360283|  3|\n",
      "|2011|    Olympic|       35.040062|  1|\n",
      "|2011|    Rampart|        32.49645|  2|\n",
      "|2011|     Harbor|       28.513363|  3|\n",
      "|2012|    Olympic|       34.297085|  1|\n",
      "|2012|    Rampart|       32.460003|  2|\n",
      "|2012|     Harbor|       29.509586|  3|\n",
      "|2013|    Olympic|        33.58218|  1|\n",
      "|2013|    Rampart|       32.106037|  2|\n",
      "|2013|     Harbor|       29.723639|  3|\n",
      "|2014|   Van Nuys|       32.021523|  1|\n",
      "|2014|West Valley|       31.497547|  2|\n",
      "|2014|    Mission|        31.22494|  3|\n",
      "|2015|   Van Nuys|        32.26514|  1|\n",
      "|2015|    Mission|       30.463762|  2|\n",
      "|2015|   Foothill|       30.353003|  3|\n",
      "|2016|   Van Nuys|        32.19452|  1|\n",
      "|2016|West Valley|       31.401464|  2|\n",
      "|2016|   Foothill|       29.908648|  3|\n",
      "|2017|   Van Nuys|       32.055428|  1|\n",
      "|2017|    Mission|       31.055387|  2|\n",
      "|2017|   Foothill|         30.4697|  3|\n",
      "|2018|   Foothill|       30.731346|  1|\n",
      "|2018|    Mission|       30.727024|  2|\n",
      "|2018|   Van Nuys|       28.905207|  3|\n",
      "|2019|    Mission|       30.727411|  1|\n",
      "|2019|West Valley|       30.579742|  2|\n",
      "|2019|N Hollywood|       29.238087|  3|\n",
      "|2020|West Valley|       30.771132|  1|\n",
      "|2020|    Mission|       30.149746|  2|\n",
      "|2020|     Harbor|       29.693487|  3|\n",
      "|2021|    Mission|       30.318115|  1|\n",
      "|2021|West Valley|       28.971087|  2|\n",
      "|2021|   Foothill|       27.993757|  3|\n",
      "|2022|West Valley|       26.536367|  1|\n",
      "|2022|     Harbor|       26.337538|  2|\n",
      "|2022|    Topanga|       26.234013|  3|\n",
      "|2023|   Foothill|        26.76076|  1|\n",
      "|2023|    Topanga|       26.538023|  2|\n",
      "|2023|    Mission|       25.662731|  3|\n",
      "|2024|N Hollywood|        19.59853|  1|\n",
      "|2024|   Foothill|       18.620882|  2|\n",
      "|2024|77th Street|       17.586319|  3|\n",
      "+----+-----------+----------------+---+\n",
      "\n",
      "Time taken by query2_dataframe: 14.42 seconds\n",
      "Time taken by query2_parquet: 14.84 seconds"
     ]
    }
   ],
   "source": [
    "# Call the function that loads DF from the single parquet (1 partition) and then executes the query \n",
    "query2_parquet(path = s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eff37ae-ca05-4fa3-9b14-f3bf26e65f74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+---+\n",
      "|year|   precinct|closed_case_rate|  #|\n",
      "+----+-----------+----------------+---+\n",
      "|2010|    Rampart|       32.847134|  1|\n",
      "|2010|    Olympic|        31.51529|  2|\n",
      "|2010|     Harbor|       29.360283|  3|\n",
      "|2011|    Olympic|       35.040062|  1|\n",
      "|2011|    Rampart|        32.49645|  2|\n",
      "|2011|     Harbor|       28.513363|  3|\n",
      "|2012|    Olympic|       34.297085|  1|\n",
      "|2012|    Rampart|       32.460003|  2|\n",
      "|2012|     Harbor|       29.509586|  3|\n",
      "|2013|    Olympic|        33.58218|  1|\n",
      "|2013|    Rampart|       32.106037|  2|\n",
      "|2013|     Harbor|       29.723639|  3|\n",
      "|2014|   Van Nuys|       32.021523|  1|\n",
      "|2014|West Valley|       31.497547|  2|\n",
      "|2014|    Mission|        31.22494|  3|\n",
      "|2015|   Van Nuys|        32.26514|  1|\n",
      "|2015|    Mission|       30.463762|  2|\n",
      "|2015|   Foothill|       30.353003|  3|\n",
      "|2016|   Van Nuys|        32.19452|  1|\n",
      "|2016|West Valley|       31.401464|  2|\n",
      "|2016|   Foothill|       29.908648|  3|\n",
      "|2017|   Van Nuys|       32.055428|  1|\n",
      "|2017|    Mission|       31.055387|  2|\n",
      "|2017|   Foothill|         30.4697|  3|\n",
      "|2018|   Foothill|       30.731346|  1|\n",
      "|2018|    Mission|       30.727024|  2|\n",
      "|2018|   Van Nuys|       28.905207|  3|\n",
      "|2019|    Mission|       30.727411|  1|\n",
      "|2019|West Valley|       30.579742|  2|\n",
      "|2019|N Hollywood|       29.238087|  3|\n",
      "|2020|West Valley|       30.771132|  1|\n",
      "|2020|    Mission|       30.149746|  2|\n",
      "|2020|     Harbor|       29.693487|  3|\n",
      "|2021|    Mission|       30.318115|  1|\n",
      "|2021|West Valley|       28.971087|  2|\n",
      "|2021|   Foothill|       27.993757|  3|\n",
      "|2022|West Valley|       26.536367|  1|\n",
      "|2022|     Harbor|       26.337538|  2|\n",
      "|2022|    Topanga|       26.234013|  3|\n",
      "|2023|   Foothill|        26.76076|  1|\n",
      "|2023|    Topanga|       26.538023|  2|\n",
      "|2023|    Mission|       25.662731|  3|\n",
      "|2024|N Hollywood|        19.59853|  1|\n",
      "|2024|   Foothill|       18.620882|  2|\n",
      "|2024|77th Street|       17.586319|  3|\n",
      "+----+-----------+----------------+---+\n",
      "\n",
      "Time taken by query2_dataframe: 5.23 seconds\n",
      "Time taken by query2_parquet: 5.66 seconds"
     ]
    }
   ],
   "source": [
    "# Call the function that loads DF from multiple parquet files and then executes the query \n",
    "query2_parquet(path = s3_path_multiple_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c25151-a991-4f2c-977f-6d65f4ebbbd1",
   "metadata": {},
   "source": [
    "## Query 2 - SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a041b4c-20c4-49ac-b481-8169a3cfeb57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@measure_time\n",
    "def query2_sql(df, debug = False):\n",
    "    '''Returns the DF that is result of the query 2 using SQL API'''\n",
    "\n",
    "    df.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT `DR_NO`, `DATE OCC`, `AREA NAME`, `Status Desc`, extract_year(`DATE OCC`) as year, `AREA NAME` as precinct, is_closed_case(`Status Desc`) as is_closed_case \n",
    "        FROM crimes\n",
    "    \"\"\"\n",
    "\n",
    "    modified_crimes = spark.sql(query)\n",
    "    if debug: modified_crimes.show(3)\n",
    "    modified_crimes.createOrReplaceTempView(\"modified_crimes\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT year, precinct, count(*) as total_cases, CAST(sum(is_closed_case) AS INT) as closed_cases, CAST(percentage(`closed_cases`,`total_cases`) AS DECIMAL(10,6)) as closed_case_rate\n",
    "        FROM modified_crimes\n",
    "        GROUP BY year, precinct\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_crimes = spark.sql(query)\n",
    "    if debug: grouped_crimes.show(3)\n",
    "    grouped_crimes.createOrReplaceTempView(\"grouped_crimes\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT year, precinct,  total_cases, closed_cases, closed_case_rate, DENSE_RANK() OVER(PARTITION BY year ORDER BY closed_case_rate DESC) as `#`\n",
    "        FROM grouped_crimes\n",
    "        ORDER BY year, closed_case_rate DESC\n",
    "    \"\"\"\n",
    "\n",
    "    ranked_crimes = spark.sql(query)\n",
    "    if debug: ranked_crimes.show(3)\n",
    "    ranked_crimes.createOrReplaceTempView(\"ranked_crimes\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT year, precinct, closed_case_rate, `#`\n",
    "        FROM ranked_crimes\n",
    "        WHERE `#` <= 3\n",
    "        ORDER BY year, `#`\n",
    "    \"\"\"\n",
    "\n",
    "    result = spark.sql(query)\n",
    "    result.show(50)\n",
    "    result.createOrReplaceTempView(\"result\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee822-b8a8-4da2-a8ae-2d61b2c91ed1",
   "metadata": {},
   "source": [
    "### Comparison of query execution time using DataFrame or SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "834d7b90-bbf9-4641-9be5-001700209a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+---+\n",
      "|year|   precinct|closed_case_rate|  #|\n",
      "+----+-----------+----------------+---+\n",
      "|2010|    Rampart|       32.847134|  1|\n",
      "|2010|    Olympic|        31.51529|  2|\n",
      "|2010|     Harbor|       29.360283|  3|\n",
      "|2011|    Olympic|       35.040062|  1|\n",
      "|2011|    Rampart|        32.49645|  2|\n",
      "|2011|     Harbor|       28.513363|  3|\n",
      "|2012|    Olympic|       34.297085|  1|\n",
      "|2012|    Rampart|       32.460003|  2|\n",
      "|2012|     Harbor|       29.509586|  3|\n",
      "|2013|    Olympic|        33.58218|  1|\n",
      "|2013|    Rampart|       32.106037|  2|\n",
      "|2013|     Harbor|       29.723639|  3|\n",
      "|2014|   Van Nuys|       32.021523|  1|\n",
      "|2014|West Valley|       31.497547|  2|\n",
      "|2014|    Mission|        31.22494|  3|\n",
      "|2015|   Van Nuys|        32.26514|  1|\n",
      "|2015|    Mission|       30.463762|  2|\n",
      "|2015|   Foothill|       30.353003|  3|\n",
      "|2016|   Van Nuys|        32.19452|  1|\n",
      "|2016|West Valley|       31.401464|  2|\n",
      "|2016|   Foothill|       29.908648|  3|\n",
      "|2017|   Van Nuys|       32.055428|  1|\n",
      "|2017|    Mission|       31.055387|  2|\n",
      "|2017|   Foothill|         30.4697|  3|\n",
      "|2018|   Foothill|       30.731346|  1|\n",
      "|2018|    Mission|       30.727024|  2|\n",
      "|2018|   Van Nuys|       28.905207|  3|\n",
      "|2019|    Mission|       30.727411|  1|\n",
      "|2019|West Valley|       30.579742|  2|\n",
      "|2019|N Hollywood|       29.238087|  3|\n",
      "|2020|West Valley|       30.771132|  1|\n",
      "|2020|    Mission|       30.149746|  2|\n",
      "|2020|     Harbor|       29.693487|  3|\n",
      "|2021|    Mission|       30.318115|  1|\n",
      "|2021|West Valley|       28.971087|  2|\n",
      "|2021|   Foothill|       27.993757|  3|\n",
      "|2022|West Valley|       26.536367|  1|\n",
      "|2022|     Harbor|       26.337538|  2|\n",
      "|2022|    Topanga|       26.234013|  3|\n",
      "|2023|   Foothill|        26.76076|  1|\n",
      "|2023|    Topanga|       26.538023|  2|\n",
      "|2023|    Mission|       25.662731|  3|\n",
      "|2024|N Hollywood|        19.59853|  1|\n",
      "|2024|   Foothill|       18.620882|  2|\n",
      "|2024|77th Street|       17.586319|  3|\n",
      "+----+-----------+----------------+---+\n",
      "\n",
      "Time taken by query2_dataframe: 9.58 seconds"
     ]
    }
   ],
   "source": [
    "# Execute the Query 2 with the DF API\n",
    "query2_dataframe(crimes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa71cdd6-0c49-41e3-9153-27bf9bb77b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------------+---+\n",
      "|year|   precinct|closed_case_rate|  #|\n",
      "+----+-----------+----------------+---+\n",
      "|2010|    Rampart|       32.847134|  1|\n",
      "|2010|    Olympic|       31.515290|  2|\n",
      "|2010|     Harbor|       29.360283|  3|\n",
      "|2011|    Olympic|       35.040060|  1|\n",
      "|2011|    Rampart|       32.496447|  2|\n",
      "|2011|     Harbor|       28.513362|  3|\n",
      "|2012|    Olympic|       34.297085|  1|\n",
      "|2012|    Rampart|       32.460005|  2|\n",
      "|2012|     Harbor|       29.509586|  3|\n",
      "|2013|    Olympic|       33.582179|  1|\n",
      "|2013|    Rampart|       32.106038|  2|\n",
      "|2013|     Harbor|       29.723639|  3|\n",
      "|2014|   Van Nuys|       32.021524|  1|\n",
      "|2014|West Valley|       31.497548|  2|\n",
      "|2014|    Mission|       31.224940|  3|\n",
      "|2015|   Van Nuys|       32.265141|  1|\n",
      "|2015|    Mission|       30.463763|  2|\n",
      "|2015|   Foothill|       30.353002|  3|\n",
      "|2016|   Van Nuys|       32.194518|  1|\n",
      "|2016|West Valley|       31.401464|  2|\n",
      "|2016|   Foothill|       29.908647|  3|\n",
      "|2017|   Van Nuys|       32.055427|  1|\n",
      "|2017|    Mission|       31.055387|  2|\n",
      "|2017|   Foothill|       30.469701|  3|\n",
      "|2018|   Foothill|       30.731347|  1|\n",
      "|2018|    Mission|       30.727023|  2|\n",
      "|2018|   Van Nuys|       28.905207|  3|\n",
      "|2019|    Mission|       30.727411|  1|\n",
      "|2019|West Valley|       30.579743|  2|\n",
      "|2019|N Hollywood|       29.238087|  3|\n",
      "|2020|West Valley|       30.771132|  1|\n",
      "|2020|    Mission|       30.149746|  2|\n",
      "|2020|     Harbor|       29.693487|  3|\n",
      "|2021|    Mission|       30.318116|  1|\n",
      "|2021|West Valley|       28.971087|  2|\n",
      "|2021|   Foothill|       27.993757|  3|\n",
      "|2022|West Valley|       26.536367|  1|\n",
      "|2022|     Harbor|       26.337538|  2|\n",
      "|2022|    Topanga|       26.234013|  3|\n",
      "|2023|   Foothill|       26.760760|  1|\n",
      "|2023|    Topanga|       26.538023|  2|\n",
      "|2023|    Mission|       25.662731|  3|\n",
      "|2024|N Hollywood|       19.598529|  1|\n",
      "|2024|   Foothill|       18.620882|  2|\n",
      "|2024|77th Street|       17.586318|  3|\n",
      "+----+-----------+----------------+---+\n",
      "\n",
      "Time taken by query2_sql: 7.52 seconds"
     ]
    }
   ],
   "source": [
    "# Execute Query 2 with the SQL API\n",
    "query2_sql(crimes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f3daa-04d0-4a4c-81cd-351037941b2d",
   "metadata": {},
   "source": [
    "# Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6445381e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st join - optimizer: 10.63 seconds\n",
      "2nd join - optimizer: 25.65 seconds\n",
      "3rd join - optimizer: 24.33 seconds\n",
      "+-------------------+------------------------+-------------------+\n",
      "|               COMM|median_income_per_person|  crimes_per_person|\n",
      "+-------------------+------------------------+-------------------+\n",
      "|  Pacific Palisades|        70526.2203104497|0.31066220995010413|\n",
      "|      Beverly Crest|       66513.90150799365| 0.2667541629070626|\n",
      "|   Marina Peninsula|       65235.69402813004|0.40050726308508183|\n",
      "|Palisades Highlands|       65048.95354904471|0.12914166449256456|\n",
      "|            Bel Air|       63259.97685510228| 0.2881007141992495|\n",
      "|  Mandeville Canyon|       61443.86522911051|  0.193628209093721|\n",
      "|          Brentwood|      60696.777650004915|0.34619978840312615|\n",
      "|            Carthay|      50282.692104378286| 0.5788834029624003|\n",
      "|             Venice|       46575.69192582585| 0.8405823754789272|\n",
      "|       Century City|       45707.53601562712| 0.5460891505466778|\n",
      "|      Playa Del Rey|         45522.596580114|  0.525965801139962|\n",
      "|        Playa Vista|      44472.100292884345|0.48532377324669507|\n",
      "|    Hollywood Hills|      43713.597155829746| 0.5432514787596343|\n",
      "|        Studio City|       42206.35394275496| 0.6427570883446844|\n",
      "|   West Los Angeles|       40983.06782689424|  0.505868578955071|\n",
      "|      South Carthay|      39642.419795898146|0.47934211829981177|\n",
      "|             Encino|       39546.65508835928| 0.4679921603815911|\n",
      "|       Miracle Mile|       38981.93388699816| 0.5439372236407797|\n",
      "|        Rancho Park|      38740.063860206516|  0.864813343923749|\n",
      "|     Woodland Hills|      38153.839762249285| 0.5016581914658412|\n",
      "+-------------------+------------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, regexp_replace, count\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 3\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flat_census_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "LA_areas = flat_census_data.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Read csvs\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "census_data = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Join\n",
    "joined_df = LA_areas.join(income_data, LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "joined_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"1st join - optimizer: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "#s3_path = \"s3://groups-bucket-dblab-905418150721/group46/query3/join.parquet\"\n",
    "#result = [(\"broadcast\", execution_time)]\n",
    "#result_df = spark.createDataFrame(result, [\"Join Strategy\", \"Execution Time (s)\"])\n",
    "#result_df.write.mode(\"append\").parquet(s3_path)\n",
    "#joined_df.explain(mode=\"formatted\")\n",
    "\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[\\\\$,]\", \"\"))\n",
    "    \n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    col(\"Estimated Median Income\").cast(\"double\")\n",
    ")\n",
    "\n",
    "LA_areas = joined_df.groupBy(\"COMM\").agg(\n",
    "                spark_sum(\"POP_2010\").alias(\"total_population\"),\n",
    "                spark_sum(\"HOUSING10\").alias(\"total_housing\"),\n",
    "                avg(\"Estimated Median Income\").alias(\"average_income_per_house\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\"))\n",
    "\n",
    "# Compute median income per person\n",
    "result_df = LA_areas.withColumn(\n",
    "    \"median_income_per_person\",\n",
    "    (col(\"total_housing\") * col(\"average_income_per_house\")) / col(\"total_population\")\n",
    ")\n",
    "\n",
    "crime_data = crime_data.withColumn(\"geometry\", ST_Point(\"LON\", \"LAT\"))\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df, ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"2nd join - optimizer: {elapsed_time:.2f} seconds\")\n",
    "# Join no.2\n",
    "#joined_crimes_df.explain(mode=\"formatted\")\n",
    "\n",
    "crime_counts = joined_crimes_df.groupBy(\"COMM\").agg(\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ")\n",
    "\n",
    "# Join no.3\n",
    "start_time = time.time()\n",
    "joined_LA = result_df.join(crime_counts, \"COMM\", \"inner\")\n",
    "joined_LA.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"3rd join - optimizer: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "final_LA_df = joined_LA.withColumn(\n",
    "    \"crimes_per_person\",\n",
    "    (col(\"crime_count\") / col(\"total_population\")\n",
    "))\n",
    "final_LA_df.select(\"COMM\", \"median_income_per_person\", \"crimes_per_person\").orderBy(col(\"median_income_per_person\").desc()).show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "\n",
    "#elapsed_time = end_time - start_time\n",
    "#print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce82ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>173</td><td>application_1738075734771_0174</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0174/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0174_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Trying different join strategies\n",
    "\n",
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, regexp_replace, count\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 3\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flat_census_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "LA_areas = flat_census_data.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Read csvs\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "census_data = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950f8aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (10)\n",
      "+- BroadcastHashJoin Inner BuildRight (9)\n",
      "   :- Project (5)\n",
      "   :  +- Filter (4)\n",
      "   :     +- Generate (3)\n",
      "   :        +- Filter (2)\n",
      "   :           +- Scan geojson  (1)\n",
      "   +- BroadcastExchange (8)\n",
      "      +- Filter (7)\n",
      "         +- Scan csv  (6)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [26]: [features#33.properties.BG10 AS BG10#42, features#33.properties.BG10FIP10 AS BG10FIP10#43, features#33.properties.BG12 AS BG12#44, features#33.properties.CB10 AS CB10#45, features#33.properties.CEN_FIP13 AS CEN_FIP13#46, features#33.properties.CITY AS CITY#47, features#33.properties.CITYCOM AS CITYCOM#48, features#33.properties.COMM AS COMM#49, features#33.properties.CT10 AS CT10#50, features#33.properties.CT12 AS CT12#51, features#33.properties.CTCB10 AS CTCB10#52, features#33.properties.HD_2012 AS HD_2012#53L, features#33.properties.HD_NAME AS HD_NAME#54, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.LA_FIP10 AS LA_FIP10#56, features#33.properties.OBJECTID AS OBJECTID#57L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.PUMA10 AS PUMA10#59, features#33.properties.SPA_2012 AS SPA_2012#60L, features#33.properties.SPA_NAME AS SPA_NAME#61, features#33.properties.SUP_DIST AS SUP_DIST#62, features#33.properties.SUP_LABEL AS SUP_LABEL#63, features#33.properties.ShapeSTArea AS ShapeSTArea#64, features#33.properties.ShapeSTLength AS ShapeSTLength#65, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=296]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) AdaptiveSparkPlan\n",
      "Output [29]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36, Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "1st join: 11.96 seconds"
     ]
    }
   ],
   "source": [
    "### First Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df = LA_areas.join(income_data, LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "joined_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_df.explain(mode=\"formatted\")\n",
    "print(f\"1st join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647dad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- SortMergeJoin Inner (12)\n",
      "   :- Sort (7)\n",
      "   :  +- Exchange (6)\n",
      "   :     +- Project (5)\n",
      "   :        +- Filter (4)\n",
      "   :           +- Generate (3)\n",
      "   :              +- Filter (2)\n",
      "   :                 +- Scan geojson  (1)\n",
      "   +- Sort (11)\n",
      "      +- Exchange (10)\n",
      "         +- Filter (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [26]: [features#33.properties.BG10 AS BG10#42, features#33.properties.BG10FIP10 AS BG10FIP10#43, features#33.properties.BG12 AS BG12#44, features#33.properties.CB10 AS CB10#45, features#33.properties.CEN_FIP13 AS CEN_FIP13#46, features#33.properties.CITY AS CITY#47, features#33.properties.CITYCOM AS CITYCOM#48, features#33.properties.COMM AS COMM#49, features#33.properties.CT10 AS CT10#50, features#33.properties.CT12 AS CT12#51, features#33.properties.CTCB10 AS CTCB10#52, features#33.properties.HD_2012 AS HD_2012#53L, features#33.properties.HD_NAME AS HD_NAME#54, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.LA_FIP10 AS LA_FIP10#56, features#33.properties.OBJECTID AS OBJECTID#57L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.PUMA10 AS PUMA10#59, features#33.properties.SPA_2012 AS SPA_2012#60L, features#33.properties.SPA_NAME AS SPA_NAME#61, features#33.properties.SUP_DIST AS SUP_DIST#62, features#33.properties.SUP_LABEL AS SUP_LABEL#63, features#33.properties.ShapeSTArea AS ShapeSTArea#64, features#33.properties.ShapeSTLength AS ShapeSTLength#65, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Exchange\n",
      "Input [26]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36]\n",
      "Arguments: hashpartitioning(cast(ZCTA10#66 as int), 1000), ENSURE_REQUIREMENTS, [plan_id=368]\n",
      "\n",
      "(7) Sort\n",
      "Input [26]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36]\n",
      "Arguments: [cast(ZCTA10#66 as int) ASC NULLS FIRST], false, 0\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(9) Filter\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(10) Exchange\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: hashpartitioning(Zip Code#278, 1000), ENSURE_REQUIREMENTS, [plan_id=369]\n",
      "\n",
      "(11) Sort\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: [Zip Code#278 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(12) SortMergeJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [29]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36, Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "1st join: 13.14 seconds"
     ]
    }
   ],
   "source": [
    "### First Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df = LA_areas.join(income_data.hint(\"MERGE\"), LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "joined_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_df.explain(mode=\"formatted\")\n",
    "print(f\"1st join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae7478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct Inner (8)\n",
      ":- * Project (5)\n",
      ":  +- * Filter (4)\n",
      ":     +- * Generate (3)\n",
      ":        +- * Filter (2)\n",
      ":           +- Scan geojson  (1)\n",
      "+- * Filter (7)\n",
      "   +- Scan csv  (6)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate [codegen id : 1]\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter [codegen id : 1]\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(5) Project [codegen id : 1]\n",
      "Output [26]: [features#33.properties.BG10 AS BG10#42, features#33.properties.BG10FIP10 AS BG10FIP10#43, features#33.properties.BG12 AS BG12#44, features#33.properties.CB10 AS CB10#45, features#33.properties.CEN_FIP13 AS CEN_FIP13#46, features#33.properties.CITY AS CITY#47, features#33.properties.CITYCOM AS CITYCOM#48, features#33.properties.COMM AS COMM#49, features#33.properties.CT10 AS CT10#50, features#33.properties.CT12 AS CT12#51, features#33.properties.CTCB10 AS CTCB10#52, features#33.properties.HD_2012 AS HD_2012#53L, features#33.properties.HD_NAME AS HD_NAME#54, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.LA_FIP10 AS LA_FIP10#56, features#33.properties.OBJECTID AS OBJECTID#57L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.PUMA10 AS PUMA10#59, features#33.properties.SPA_2012 AS SPA_2012#60L, features#33.properties.SPA_NAME AS SPA_NAME#61, features#33.properties.SUP_DIST AS SUP_DIST#62, features#33.properties.SUP_LABEL AS SUP_LABEL#63, features#33.properties.ShapeSTArea AS ShapeSTArea#64, features#33.properties.ShapeSTLength AS ShapeSTLength#65, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter [codegen id : 2]\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (cast(ZCTA10#66 as int) = Zip Code#278)\n",
      "\n",
      "\n",
      "1st join: 12.77 seconds"
     ]
    }
   ],
   "source": [
    "### First Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df = LA_areas.join(income_data.hint(\"SHUFFLE_REPLICATE_NL\"), LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "joined_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_df.explain(mode=\"formatted\")\n",
    "print(f\"1st join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a2dacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- ShuffledHashJoin Inner BuildRight (10)\n",
      "   :- Exchange (6)\n",
      "   :  +- Project (5)\n",
      "   :     +- Filter (4)\n",
      "   :        +- Generate (3)\n",
      "   :           +- Filter (2)\n",
      "   :              +- Scan geojson  (1)\n",
      "   +- Exchange (9)\n",
      "      +- Filter (8)\n",
      "         +- Scan csv  (7)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [26]: [features#33.properties.BG10 AS BG10#42, features#33.properties.BG10FIP10 AS BG10FIP10#43, features#33.properties.BG12 AS BG12#44, features#33.properties.CB10 AS CB10#45, features#33.properties.CEN_FIP13 AS CEN_FIP13#46, features#33.properties.CITY AS CITY#47, features#33.properties.CITYCOM AS CITYCOM#48, features#33.properties.COMM AS COMM#49, features#33.properties.CT10 AS CT10#50, features#33.properties.CT12 AS CT12#51, features#33.properties.CTCB10 AS CTCB10#52, features#33.properties.HD_2012 AS HD_2012#53L, features#33.properties.HD_NAME AS HD_NAME#54, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.LA_FIP10 AS LA_FIP10#56, features#33.properties.OBJECTID AS OBJECTID#57L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.PUMA10 AS PUMA10#59, features#33.properties.SPA_2012 AS SPA_2012#60L, features#33.properties.SPA_NAME AS SPA_NAME#61, features#33.properties.SUP_DIST AS SUP_DIST#62, features#33.properties.SUP_LABEL AS SUP_LABEL#63, features#33.properties.ShapeSTArea AS ShapeSTArea#64, features#33.properties.ShapeSTLength AS ShapeSTLength#65, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Exchange\n",
      "Input [26]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36]\n",
      "Arguments: hashpartitioning(cast(ZCTA10#66 as int), 1000), ENSURE_REQUIREMENTS, [plan_id=322]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(8) Filter\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(9) Exchange\n",
      "Input [3]: [Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: hashpartitioning(Zip Code#278, 1000), ENSURE_REQUIREMENTS, [plan_id=323]\n",
      "\n",
      "(10) ShuffledHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [29]: [BG10#42, BG10FIP10#43, BG12#44, CB10#45, CEN_FIP13#46, CITY#47, CITYCOM#48, COMM#49, CT10#50, CT12#51, CTCB10#52, HD_2012#53L, HD_NAME#54, HOUSING10#55L, LA_FIP10#56, OBJECTID#57L, POP_2010#58L, PUMA10#59, SPA_2012#60L, SPA_NAME#61, SUP_DIST#62, SUP_LABEL#63, ShapeSTArea#64, ShapeSTLength#65, ZCTA10#66, geometry#36, Zip Code#278, Community#279, Estimated Median Income#280]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "1st join: 12.47 seconds"
     ]
    }
   ],
   "source": [
    "### First Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df = LA_areas.join(income_data.hint(\"SHUFFLE_HASH\"), LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "joined_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_df.explain(mode=\"formatted\")\n",
    "print(f\"1st join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d392651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_df = joined_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[\\\\$,]\", \"\"))\n",
    "    \n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    col(\"Estimated Median Income\").cast(\"double\")\n",
    ")\n",
    "\n",
    "LA_areas = joined_df.groupBy(\"COMM\").agg(\n",
    "                spark_sum(\"POP_2010\").alias(\"total_population\"),\n",
    "                spark_sum(\"HOUSING10\").alias(\"total_housing\"),\n",
    "                avg(\"Estimated Median Income\").alias(\"average_income_per_house\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\"))\n",
    "\n",
    "# Compute median income per person\n",
    "result_df = LA_areas.withColumn(\n",
    "    \"median_income_per_person\",\n",
    "    (col(\"total_housing\") * col(\"average_income_per_house\")) / col(\"total_population\")\n",
    ")\n",
    "\n",
    "crime_data = crime_data.withColumn(\"geometry\", ST_Point(\"LON\", \"LAT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffba6aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (20)\n",
      "+- RangeJoin (19)\n",
      "   :- Project (3)\n",
      "   :  +- Filter (2)\n",
      "   :     +- Scan csv  (1)\n",
      "   +- Project (18)\n",
      "      +- Filter (17)\n",
      "         +- ObjectHashAggregate (16)\n",
      "            +- Exchange (15)\n",
      "               +- ObjectHashAggregate (14)\n",
      "                  +- Project (13)\n",
      "                     +- BroadcastHashJoin Inner BuildRight (12)\n",
      "                        :- Project (8)\n",
      "                        :  +- Filter (7)\n",
      "                        :     +- Generate (6)\n",
      "                        :        +- Filter (5)\n",
      "                        :           +- Scan geojson  (4)\n",
      "                        +- BroadcastExchange (11)\n",
      "                           +- Filter (10)\n",
      "                              +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "\n",
      "(4) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(6) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(7) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=783]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(15) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(17) Filter\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(19) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(20) AdaptiveSparkPlan\n",
      "Output [35]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215, geometry#510, COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "2nd join: 23.44 seconds"
     ]
    }
   ],
   "source": [
    "### Second Join\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df, ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_crimes_df.explain(mode=\"formatted\")\n",
    "print(f\"2nd join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ec5414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (21)\n",
      "+- BroadcastIndexJoin (20)\n",
      "   :- Project (3)\n",
      "   :  +- Filter (2)\n",
      "   :     +- Scan csv  (1)\n",
      "   +- SpatialIndex (19)\n",
      "      +- Project (18)\n",
      "         +- Filter (17)\n",
      "            +- ObjectHashAggregate (16)\n",
      "               +- Exchange (15)\n",
      "                  +- ObjectHashAggregate (14)\n",
      "                     +- Project (13)\n",
      "                        +- BroadcastHashJoin Inner BuildRight (12)\n",
      "                           :- Project (8)\n",
      "                           :  +- Filter (7)\n",
      "                           :     +- Generate (6)\n",
      "                           :        +- Filter (5)\n",
      "                           :           +- Scan geojson  (4)\n",
      "                           +- BroadcastExchange (11)\n",
      "                              +- Filter (10)\n",
      "                                 +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "\n",
      "(4) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(6) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(7) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=800]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8517a87, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(15) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=805]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8517a87, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(17) Filter\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(19) SpatialIndex\n",
      "Arguments: geometry#485: geometry, RTREE, false, false\n",
      "\n",
      "(20) BroadcastIndexJoin\n",
      "Arguments: geometry#510: geometry, RightSide, LeftSide, Inner, WITHIN\n",
      "\n",
      "(21) AdaptiveSparkPlan\n",
      "Output [35]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215, geometry#510, COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "2nd join: 25.67 seconds"
     ]
    }
   ],
   "source": [
    "### Second Join\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df.hint(\"BROADCAST\"), ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_crimes_df.explain(mode=\"formatted\")\n",
    "print(f\"2nd join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f6e87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (20)\n",
      "+- RangeJoin (19)\n",
      "   :- Project (3)\n",
      "   :  +- Filter (2)\n",
      "   :     +- Scan csv  (1)\n",
      "   +- Project (18)\n",
      "      +- Filter (17)\n",
      "         +- ObjectHashAggregate (16)\n",
      "            +- Exchange (15)\n",
      "               +- ObjectHashAggregate (14)\n",
      "                  +- Project (13)\n",
      "                     +- BroadcastHashJoin Inner BuildRight (12)\n",
      "                        :- Project (8)\n",
      "                        :  +- Filter (7)\n",
      "                        :     +- Generate (6)\n",
      "                        :        +- Filter (5)\n",
      "                        :           +- Scan geojson  (4)\n",
      "                        +- BroadcastExchange (11)\n",
      "                           +- Filter (10)\n",
      "                              +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "\n",
      "(4) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(6) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(7) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=783]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@10b93ee1, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(15) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@10b93ee1, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(17) Filter\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(19) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(20) AdaptiveSparkPlan\n",
      "Output [35]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215, geometry#510, COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "2nd join: 25.36 seconds"
     ]
    }
   ],
   "source": [
    "### Second Join\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df.hint(\"SHUFFLE_HASH\"), ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_crimes_df.explain(mode=\"formatted\")\n",
    "print(f\"2nd join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2c972a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (20)\n",
      "+- RangeJoin (19)\n",
      "   :- Project (3)\n",
      "   :  +- Filter (2)\n",
      "   :     +- Scan csv  (1)\n",
      "   +- Project (18)\n",
      "      +- Filter (17)\n",
      "         +- ObjectHashAggregate (16)\n",
      "            +- Exchange (15)\n",
      "               +- ObjectHashAggregate (14)\n",
      "                  +- Project (13)\n",
      "                     +- BroadcastHashJoin Inner BuildRight (12)\n",
      "                        :- Project (8)\n",
      "                        :  +- Filter (7)\n",
      "                        :     +- Generate (6)\n",
      "                        :        +- Filter (5)\n",
      "                        :           +- Scan geojson  (4)\n",
      "                        +- BroadcastExchange (11)\n",
      "                           +- Filter (10)\n",
      "                              +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "\n",
      "(4) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(6) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(7) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=783]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@b5e07b0, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(15) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@b5e07b0, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(17) Filter\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(19) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(20) AdaptiveSparkPlan\n",
      "Output [35]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215, geometry#510, COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "2nd join: 22.49 seconds"
     ]
    }
   ],
   "source": [
    "### Second Join\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df.hint(\"SHUFFLE_REPLICATE_NL\"), ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_crimes_df.explain(mode=\"formatted\")\n",
    "print(f\"2nd join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22de15de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (20)\n",
      "+- RangeJoin (19)\n",
      "   :- Project (3)\n",
      "   :  +- Filter (2)\n",
      "   :     +- Scan csv  (1)\n",
      "   +- Project (18)\n",
      "      +- Filter (17)\n",
      "         +- ObjectHashAggregate (16)\n",
      "            +- Exchange (15)\n",
      "               +- ObjectHashAggregate (14)\n",
      "                  +- Project (13)\n",
      "                     +- BroadcastHashJoin Inner BuildRight (12)\n",
      "                        :- Project (8)\n",
      "                        :  +- Filter (7)\n",
      "                        :     +- Generate (6)\n",
      "                        :        +- Filter (5)\n",
      "                        :           +- Scan geojson  (4)\n",
      "                        +- BroadcastExchange (11)\n",
      "                           +- Filter (10)\n",
      "                              +- Scan csv  (9)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dist No:int,Part 1-2:int,Crm Cd:int,Crm Cd Desc:string,Mocodes:string,Vict Age:int,Vict Sex:string,Vict Descent:string,Premis Cd:int,Premis Desc:string,Weapon Used Cd:int,Weapon Desc:string,Status:string,Status Desc:string,Crm Cd 1:int,Crm Cd 2:int,Crm Cd 3:int,Crm Cd 4:int,LOCATION:string,Cross Street:string,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [29]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [28]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215]\n",
      "\n",
      "(4) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(6) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(7) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=783]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@20e7f486, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(15) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@20e7f486, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(17) Filter\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(19) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(20) AdaptiveSparkPlan\n",
      "Output [35]: [DR_NO#188, Date Rptd#189, DATE OCC#190, TIME OCC#191, AREA #192, AREA NAME#193, Rpt Dist No#194, Part 1-2#195, Crm Cd#196, Crm Cd Desc#197, Mocodes#198, Vict Age#199, Vict Sex#200, Vict Descent#201, Premis Cd#202, Premis Desc#203, Weapon Used Cd#204, Weapon Desc#205, Status#206, Status Desc#207, Crm Cd 1#208, Crm Cd 2#209, Crm Cd 3#210, Crm Cd 4#211, LOCATION#212, Cross Street#213, LAT#214, LON#215, geometry#510, COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "2nd join: 21.77 seconds"
     ]
    }
   ],
   "source": [
    "### Second Join\n",
    "start_time = time.time()\n",
    "joined_crimes_df = crime_data \\\n",
    "    .join(result_df.hint(\"MERGE\"), ST_Within(crime_data.geometry, result_df.geometry), \"inner\")\n",
    "joined_crimes_df.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_crimes_df.explain(mode=\"formatted\")\n",
    "print(f\"2nd join: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10d35dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crime_counts = joined_crimes_df.groupBy(\"COMM\").agg(\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27767acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (42)\n",
      "+- Project (41)\n",
      "   +- SortMergeJoin Inner (40)\n",
      "      :- Sort (15)\n",
      "      :  +- Project (14)\n",
      "      :     +- ObjectHashAggregate (13)\n",
      "      :        +- Exchange (12)\n",
      "      :           +- ObjectHashAggregate (11)\n",
      "      :              +- Project (10)\n",
      "      :                 +- BroadcastHashJoin Inner BuildRight (9)\n",
      "      :                    :- Project (5)\n",
      "      :                    :  +- Filter (4)\n",
      "      :                    :     +- Generate (3)\n",
      "      :                    :        +- Filter (2)\n",
      "      :                    :           +- Scan geojson  (1)\n",
      "      :                    +- BroadcastExchange (8)\n",
      "      :                       +- Filter (7)\n",
      "      :                          +- Scan csv  (6)\n",
      "      +- Sort (39)\n",
      "         +- HashAggregate (38)\n",
      "            +- Exchange (37)\n",
      "               +- HashAggregate (36)\n",
      "                  +- Project (35)\n",
      "                     +- RangeJoin (34)\n",
      "                        :- Project (18)\n",
      "                        :  +- Filter (17)\n",
      "                        :     +- Scan csv  (16)\n",
      "                        +- Filter (33)\n",
      "                           +- ObjectHashAggregate (32)\n",
      "                              +- Exchange (31)\n",
      "                                 +- ObjectHashAggregate (30)\n",
      "                                    +- Filter (29)\n",
      "                                       +- Project (28)\n",
      "                                          +- BroadcastHashJoin Inner BuildRight (27)\n",
      "                                             :- Project (23)\n",
      "                                             :  +- Filter (22)\n",
      "                                             :     +- Generate (21)\n",
      "                                             :        +- Filter (20)\n",
      "                                             :           +- Scan geojson  (19)\n",
      "                                             +- BroadcastExchange (26)\n",
      "                                                +- Filter (25)\n",
      "                                                   +- Scan csv  (24)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1979]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(11) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@fdaf70f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(12) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=1984]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@fdaf70f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(14) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(15) Sort\n",
      "Input [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503]\n",
      "Arguments: [COMM#49 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "\n",
      "(19) Scan geojson \n",
      "Output [1]: [features#732]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(20) Filter\n",
      "Input [1]: [features#732]\n",
      "Condition : ((size(features#732, true) > 0) AND isnotnull(features#732))\n",
      "\n",
      "(21) Generate\n",
      "Input [1]: [features#732]\n",
      "Arguments: explode(features#732), false, [features#33]\n",
      "\n",
      "(22) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : (((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10)) AND isnotnull(features#33.properties.COMM))\n",
      "\n",
      "(23) Project\n",
      "Output [3]: [features#33.properties.COMM AS COMM#745, features#33.properties.ZCTA10 AS ZCTA10#762, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(24) Scan csv \n",
      "Output [1]: [Zip Code#735]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "(25) Filter\n",
      "Input [1]: [Zip Code#735]\n",
      "Condition : isnotnull(Zip Code#735)\n",
      "\n",
      "(26) BroadcastExchange\n",
      "Input [1]: [Zip Code#735]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1987]\n",
      "\n",
      "(27) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#762 as int)]\n",
      "Right keys [1]: [Zip Code#735]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(28) Project\n",
      "Output [2]: [COMM#745, geometry#36]\n",
      "Input [4]: [COMM#745, ZCTA10#762, geometry#36, Zip Code#735]\n",
      "\n",
      "(29) Filter\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Condition : true\n",
      "\n",
      "(30) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@fdaf70f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#651]\n",
      "Results [2]: [COMM#745, buf#652]\n",
      "\n",
      "(31) Exchange\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2010]\n",
      "\n",
      "(32) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@fdaf70f, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#36)#484]\n",
      "Results [2]: [COMM#745, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(33) Filter\n",
      "Input [2]: [COMM#745, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(34) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(35) Project\n",
      "Output [1]: [COMM#745]\n",
      "Input [3]: [geometry#510, COMM#745, geometry#485]\n",
      "\n",
      "(36) HashAggregate\n",
      "Input [1]: [COMM#745]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#818L]\n",
      "Results [2]: [COMM#745, count#819L]\n",
      "\n",
      "(37) Exchange\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2016]\n",
      "\n",
      "(38) HashAggregate\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#727L]\n",
      "Results [2]: [COMM#745, count(1)#727L AS crime_count#728L]\n",
      "\n",
      "(39) Sort\n",
      "Input [2]: [COMM#745, crime_count#728L]\n",
      "Arguments: [COMM#745 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(40) SortMergeJoin\n",
      "Left keys [1]: [COMM#49]\n",
      "Right keys [1]: [COMM#745]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(41) Project\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Input [8]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, COMM#745, crime_count#728L]\n",
      "\n",
      "(42) AdaptiveSparkPlan\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "3rd join - optimizer: 19.81 seconds"
     ]
    }
   ],
   "source": [
    "### Third Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_LA = result_df.join(crime_counts, \"COMM\", \"inner\")\n",
    "joined_LA.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_LA.explain(mode=\"formatted\")\n",
    "print(f\"3rd join - optimizer: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28af4144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (40)\n",
      "+- Project (39)\n",
      "   +- CartesianProduct Inner (38)\n",
      "      :- Project (14)\n",
      "      :  +- ObjectHashAggregate (13)\n",
      "      :     +- Exchange (12)\n",
      "      :        +- ObjectHashAggregate (11)\n",
      "      :           +- Project (10)\n",
      "      :              +- BroadcastHashJoin Inner BuildRight (9)\n",
      "      :                 :- Project (5)\n",
      "      :                 :  +- Filter (4)\n",
      "      :                 :     +- Generate (3)\n",
      "      :                 :        +- Filter (2)\n",
      "      :                 :           +- Scan geojson  (1)\n",
      "      :                 +- BroadcastExchange (8)\n",
      "      :                    +- Filter (7)\n",
      "      :                       +- Scan csv  (6)\n",
      "      +- HashAggregate (37)\n",
      "         +- Exchange (36)\n",
      "            +- HashAggregate (35)\n",
      "               +- Project (34)\n",
      "                  +- RangeJoin (33)\n",
      "                     :- Project (17)\n",
      "                     :  +- Filter (16)\n",
      "                     :     +- Scan csv  (15)\n",
      "                     +- Filter (32)\n",
      "                        +- ObjectHashAggregate (31)\n",
      "                           +- Exchange (30)\n",
      "                              +- ObjectHashAggregate (29)\n",
      "                                 +- Filter (28)\n",
      "                                    +- Project (27)\n",
      "                                       +- BroadcastHashJoin Inner BuildRight (26)\n",
      "                                          :- Project (22)\n",
      "                                          :  +- Filter (21)\n",
      "                                          :     +- Generate (20)\n",
      "                                          :        +- Filter (19)\n",
      "                                          :           +- Scan geojson  (18)\n",
      "                                          +- BroadcastExchange (25)\n",
      "                                             +- Filter (24)\n",
      "                                                +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1996]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(11) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8c92cc3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(12) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=2001]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8c92cc3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(14) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(15) Scan csv \n",
      "Output [2]: [LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(16) Filter\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(17) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "\n",
      "(18) Scan geojson \n",
      "Output [1]: [features#732]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(19) Filter\n",
      "Input [1]: [features#732]\n",
      "Condition : ((size(features#732, true) > 0) AND isnotnull(features#732))\n",
      "\n",
      "(20) Generate\n",
      "Input [1]: [features#732]\n",
      "Arguments: explode(features#732), false, [features#33]\n",
      "\n",
      "(21) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : (((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10)) AND isnotnull(features#33.properties.COMM))\n",
      "\n",
      "(22) Project\n",
      "Output [3]: [features#33.properties.COMM AS COMM#745, features#33.properties.ZCTA10 AS ZCTA10#762, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(23) Scan csv \n",
      "Output [1]: [Zip Code#735]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [Zip Code#735]\n",
      "Condition : isnotnull(Zip Code#735)\n",
      "\n",
      "(25) BroadcastExchange\n",
      "Input [1]: [Zip Code#735]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2004]\n",
      "\n",
      "(26) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#762 as int)]\n",
      "Right keys [1]: [Zip Code#735]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [COMM#745, geometry#36]\n",
      "Input [4]: [COMM#745, ZCTA10#762, geometry#36, Zip Code#735]\n",
      "\n",
      "(28) Filter\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Condition : true\n",
      "\n",
      "(29) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8c92cc3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#651]\n",
      "Results [2]: [COMM#745, buf#652]\n",
      "\n",
      "(30) Exchange\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2023]\n",
      "\n",
      "(31) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@8c92cc3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#36)#484]\n",
      "Results [2]: [COMM#745, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(32) Filter\n",
      "Input [2]: [COMM#745, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(33) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(34) Project\n",
      "Output [1]: [COMM#745]\n",
      "Input [3]: [geometry#510, COMM#745, geometry#485]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#745]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#818L]\n",
      "Results [2]: [COMM#745, count#819L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2029]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#727L]\n",
      "Results [2]: [COMM#745, count(1)#727L AS crime_count#728L]\n",
      "\n",
      "(38) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (COMM#49 = COMM#745)\n",
      "\n",
      "(39) Project\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Input [8]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, COMM#745, crime_count#728L]\n",
      "\n",
      "(40) AdaptiveSparkPlan\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "3rd join - optimizer: 20.43 seconds"
     ]
    }
   ],
   "source": [
    "### Third Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_LA = result_df.join(crime_counts.hint(\"SHUFFLE_REPLICATE_NL\"), \"COMM\", \"inner\")\n",
    "joined_LA.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_LA.explain(mode=\"formatted\")\n",
    "print(f\"3rd join - optimizer: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a34b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (40)\n",
      "+- Project (39)\n",
      "   +- ShuffledHashJoin Inner BuildRight (38)\n",
      "      :- Project (14)\n",
      "      :  +- ObjectHashAggregate (13)\n",
      "      :     +- Exchange (12)\n",
      "      :        +- ObjectHashAggregate (11)\n",
      "      :           +- Project (10)\n",
      "      :              +- BroadcastHashJoin Inner BuildRight (9)\n",
      "      :                 :- Project (5)\n",
      "      :                 :  +- Filter (4)\n",
      "      :                 :     +- Generate (3)\n",
      "      :                 :        +- Filter (2)\n",
      "      :                 :           +- Scan geojson  (1)\n",
      "      :                 +- BroadcastExchange (8)\n",
      "      :                    +- Filter (7)\n",
      "      :                       +- Scan csv  (6)\n",
      "      +- HashAggregate (37)\n",
      "         +- Exchange (36)\n",
      "            +- HashAggregate (35)\n",
      "               +- Project (34)\n",
      "                  +- RangeJoin (33)\n",
      "                     :- Project (17)\n",
      "                     :  +- Filter (16)\n",
      "                     :     +- Scan csv  (15)\n",
      "                     +- Filter (32)\n",
      "                        +- ObjectHashAggregate (31)\n",
      "                           +- Exchange (30)\n",
      "                              +- ObjectHashAggregate (29)\n",
      "                                 +- Filter (28)\n",
      "                                    +- Project (27)\n",
      "                                       +- BroadcastHashJoin Inner BuildRight (26)\n",
      "                                          :- Project (22)\n",
      "                                          :  +- Filter (21)\n",
      "                                          :     +- Generate (20)\n",
      "                                          :        +- Filter (19)\n",
      "                                          :           +- Scan geojson  (18)\n",
      "                                          +- BroadcastExchange (25)\n",
      "                                             +- Filter (24)\n",
      "                                                +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1992]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(11) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7823fa3d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(12) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=1997]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7823fa3d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(14) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(15) Scan csv \n",
      "Output [2]: [LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(16) Filter\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(17) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "\n",
      "(18) Scan geojson \n",
      "Output [1]: [features#732]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(19) Filter\n",
      "Input [1]: [features#732]\n",
      "Condition : ((size(features#732, true) > 0) AND isnotnull(features#732))\n",
      "\n",
      "(20) Generate\n",
      "Input [1]: [features#732]\n",
      "Arguments: explode(features#732), false, [features#33]\n",
      "\n",
      "(21) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : (((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10)) AND isnotnull(features#33.properties.COMM))\n",
      "\n",
      "(22) Project\n",
      "Output [3]: [features#33.properties.COMM AS COMM#745, features#33.properties.ZCTA10 AS ZCTA10#762, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(23) Scan csv \n",
      "Output [1]: [Zip Code#735]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [Zip Code#735]\n",
      "Condition : isnotnull(Zip Code#735)\n",
      "\n",
      "(25) BroadcastExchange\n",
      "Input [1]: [Zip Code#735]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2000]\n",
      "\n",
      "(26) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#762 as int)]\n",
      "Right keys [1]: [Zip Code#735]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [COMM#745, geometry#36]\n",
      "Input [4]: [COMM#745, ZCTA10#762, geometry#36, Zip Code#735]\n",
      "\n",
      "(28) Filter\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Condition : true\n",
      "\n",
      "(29) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7823fa3d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#651]\n",
      "Results [2]: [COMM#745, buf#652]\n",
      "\n",
      "(30) Exchange\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2020]\n",
      "\n",
      "(31) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7823fa3d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#36)#484]\n",
      "Results [2]: [COMM#745, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(32) Filter\n",
      "Input [2]: [COMM#745, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(33) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(34) Project\n",
      "Output [1]: [COMM#745]\n",
      "Input [3]: [geometry#510, COMM#745, geometry#485]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#745]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#818L]\n",
      "Results [2]: [COMM#745, count#819L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2026]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#727L]\n",
      "Results [2]: [COMM#745, count(1)#727L AS crime_count#728L]\n",
      "\n",
      "(38) ShuffledHashJoin\n",
      "Left keys [1]: [COMM#49]\n",
      "Right keys [1]: [COMM#745]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(39) Project\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Input [8]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, COMM#745, crime_count#728L]\n",
      "\n",
      "(40) AdaptiveSparkPlan\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "3rd join - optimizer: 21.15 seconds"
     ]
    }
   ],
   "source": [
    "### Third Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_LA = result_df.join(crime_counts.hint(\"SHUFFLE_HASH\"), \"COMM\", \"inner\")\n",
    "joined_LA.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_LA.explain(mode=\"formatted\")\n",
    "print(f\"3rd join - optimizer: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ce09c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (41)\n",
      "+- Project (40)\n",
      "   +- BroadcastHashJoin Inner BuildRight (39)\n",
      "      :- Project (14)\n",
      "      :  +- ObjectHashAggregate (13)\n",
      "      :     +- Exchange (12)\n",
      "      :        +- ObjectHashAggregate (11)\n",
      "      :           +- Project (10)\n",
      "      :              +- BroadcastHashJoin Inner BuildRight (9)\n",
      "      :                 :- Project (5)\n",
      "      :                 :  +- Filter (4)\n",
      "      :                 :     +- Generate (3)\n",
      "      :                 :        +- Filter (2)\n",
      "      :                 :           +- Scan geojson  (1)\n",
      "      :                 +- BroadcastExchange (8)\n",
      "      :                    +- Filter (7)\n",
      "      :                       +- Scan csv  (6)\n",
      "      +- BroadcastExchange (38)\n",
      "         +- HashAggregate (37)\n",
      "            +- Exchange (36)\n",
      "               +- HashAggregate (35)\n",
      "                  +- Project (34)\n",
      "                     +- RangeJoin (33)\n",
      "                        :- Project (17)\n",
      "                        :  +- Filter (16)\n",
      "                        :     +- Scan csv  (15)\n",
      "                        +- Filter (32)\n",
      "                           +- ObjectHashAggregate (31)\n",
      "                              +- Exchange (30)\n",
      "                                 +- ObjectHashAggregate (29)\n",
      "                                    +- Filter (28)\n",
      "                                       +- Project (27)\n",
      "                                          +- BroadcastHashJoin Inner BuildRight (26)\n",
      "                                             :- Project (22)\n",
      "                                             :  +- Filter (21)\n",
      "                                             :     +- Generate (20)\n",
      "                                             :        +- Filter (19)\n",
      "                                             :           +- Scan geojson  (18)\n",
      "                                             +- BroadcastExchange (25)\n",
      "                                                +- Filter (24)\n",
      "                                                   +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#25]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#25]\n",
      "Condition : ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#25]\n",
      "Arguments: explode(features#25), false, [features#33]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : ((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#33.properties.COMM AS COMM#49, features#33.properties.HOUSING10 AS HOUSING10#55L, features#33.properties.POP_2010 AS POP_2010#58L, features#33.properties.ZCTA10 AS ZCTA10#66, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Condition : isnotnull(Zip Code#278)\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [2]: [Zip Code#278, Estimated Median Income#280]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2000]\n",
      "\n",
      "(9) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#66 as int)]\n",
      "Right keys [1]: [Zip Code#278]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, cast(regexp_replace(Estimated Median Income#280, [\\$,], , 1) as double) AS Estimated Median Income#416]\n",
      "Input [7]: [COMM#49, HOUSING10#55L, POP_2010#58L, ZCTA10#66, geometry#36, Zip Code#278, Estimated Median Income#280]\n",
      "\n",
      "(11) ObjectHashAggregate\n",
      "Input [5]: [COMM#49, HOUSING10#55L, POP_2010#58L, geometry#36, Estimated Median Income#416]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [partial_sum(POP_2010#58L), partial_sum(HOUSING10#55L), partial_avg(Estimated Median Income#416), partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [5]: [sum#669L, sum#671L, sum#673, count#674L, buf#651]\n",
      "Results [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "\n",
      "(12) Exchange\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Arguments: hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=2005]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [6]: [COMM#49, sum#670L, sum#672L, sum#675, count#676L, buf#652]\n",
      "Keys [1]: [COMM#49]\n",
      "Functions [4]: [sum(POP_2010#58L), sum(HOUSING10#55L), avg(Estimated Median Income#416), st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [4]: [sum(POP_2010#58L)#475L, sum(HOUSING10#55L)#477L, avg(Estimated Median Income#416)#479, ST_Union_Aggr(geometry#36)#484]\n",
      "Results [5]: [COMM#49, sum(POP_2010#58L)#475L AS total_population#476L, sum(HOUSING10#55L)#477L AS total_housing#478L, avg(Estimated Median Income#416)#479 AS average_income_per_house#480, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(14) Project\n",
      "Output [6]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, ((cast(total_housing#478L as double) * average_income_per_house#480) / cast(total_population#476L as double)) AS median_income_per_person#503]\n",
      "Input [5]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485]\n",
      "\n",
      "(15) Scan csv \n",
      "Output [2]: [LAT#214, LON#215]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(16) Filter\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(17) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#510]\n",
      "Input [2]: [LAT#214, LON#215]\n",
      "\n",
      "(18) Scan geojson \n",
      "Output [1]: [features#732]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(19) Filter\n",
      "Input [1]: [features#732]\n",
      "Condition : ((size(features#732, true) > 0) AND isnotnull(features#732))\n",
      "\n",
      "(20) Generate\n",
      "Input [1]: [features#732]\n",
      "Arguments: explode(features#732), false, [features#33]\n",
      "\n",
      "(21) Filter\n",
      "Input [1]: [features#33]\n",
      "Condition : (((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.ZCTA10)) AND isnotnull(features#33.properties.COMM))\n",
      "\n",
      "(22) Project\n",
      "Output [3]: [features#33.properties.COMM AS COMM#745, features#33.properties.ZCTA10 AS ZCTA10#762, features#33.geometry AS geometry#36]\n",
      "Input [1]: [features#33]\n",
      "\n",
      "(23) Scan csv \n",
      "Output [1]: [Zip Code#735]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [Zip Code#735]\n",
      "Condition : isnotnull(Zip Code#735)\n",
      "\n",
      "(25) BroadcastExchange\n",
      "Input [1]: [Zip Code#735]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2008]\n",
      "\n",
      "(26) BroadcastHashJoin\n",
      "Left keys [1]: [cast(ZCTA10#762 as int)]\n",
      "Right keys [1]: [Zip Code#735]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [COMM#745, geometry#36]\n",
      "Input [4]: [COMM#745, ZCTA10#762, geometry#36, Zip Code#735]\n",
      "\n",
      "(28) Filter\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Condition : true\n",
      "\n",
      "(29) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, geometry#36]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#651]\n",
      "Results [2]: [COMM#745, buf#652]\n",
      "\n",
      "(30) Exchange\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2029]\n",
      "\n",
      "(31) ObjectHashAggregate\n",
      "Input [2]: [COMM#745, buf#652]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1f23ba79, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#36)#484]\n",
      "Results [2]: [COMM#745, ST_Union_Aggr(geometry#36)#484 AS geometry#485]\n",
      "\n",
      "(32) Filter\n",
      "Input [2]: [COMM#745, geometry#485]\n",
      "Condition : isnotnull(geometry#485)\n",
      "\n",
      "(33) RangeJoin\n",
      "Arguments: geometry#510: geometry, geometry#485: geometry, WITHIN\n",
      "\n",
      "(34) Project\n",
      "Output [1]: [COMM#745]\n",
      "Input [3]: [geometry#510, COMM#745, geometry#485]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#745]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#818L]\n",
      "Results [2]: [COMM#745, count#819L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Arguments: hashpartitioning(COMM#745, 1000), ENSURE_REQUIREMENTS, [plan_id=2035]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#745, count#819L]\n",
      "Keys [1]: [COMM#745]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#727L]\n",
      "Results [2]: [COMM#745, count(1)#727L AS crime_count#728L]\n",
      "\n",
      "(38) BroadcastExchange\n",
      "Input [2]: [COMM#745, crime_count#728L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=2037]\n",
      "\n",
      "(39) BroadcastHashJoin\n",
      "Left keys [1]: [COMM#49]\n",
      "Right keys [1]: [COMM#745]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(40) Project\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Input [8]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, COMM#745, crime_count#728L]\n",
      "\n",
      "(41) AdaptiveSparkPlan\n",
      "Output [7]: [COMM#49, total_population#476L, total_housing#478L, average_income_per_house#480, geometry#485, median_income_per_person#503, crime_count#728L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "3rd join - optimizer: 20.58 seconds"
     ]
    }
   ],
   "source": [
    "### Third Join\n",
    "\n",
    "start_time = time.time()\n",
    "joined_LA = result_df.join(crime_counts.hint(\"BROADCAST\"), \"COMM\", \"inner\")\n",
    "joined_LA.count()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "joined_LA.explain(mode=\"formatted\")\n",
    "print(f\"3rd join - optimizer: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a696bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-------------------+\n",
      "|               COMM|median_income_per_person|  crimes_per_person|\n",
      "+-------------------+------------------------+-------------------+\n",
      "|  Pacific Palisades|        70526.2203104497|0.31066220995010413|\n",
      "|      Beverly Crest|       66513.90150799365| 0.2667541629070626|\n",
      "|   Marina Peninsula|       65235.69402813004|0.40050726308508183|\n",
      "|Palisades Highlands|       65048.95354904471|0.12914166449256456|\n",
      "|            Bel Air|       63259.97685510228| 0.2881007141992495|\n",
      "|  Mandeville Canyon|       61443.86522911051|  0.193628209093721|\n",
      "|          Brentwood|      60696.777650004915|0.34619978840312615|\n",
      "|            Carthay|      50282.692104378286| 0.5788834029624003|\n",
      "|             Venice|       46575.69192582585| 0.8405823754789272|\n",
      "|       Century City|       45707.53601562712| 0.5460891505466778|\n",
      "|      Playa Del Rey|         45522.596580114|  0.525965801139962|\n",
      "|        Playa Vista|      44472.100292884345|0.48532377324669507|\n",
      "|    Hollywood Hills|      43713.597155829746| 0.5432514787596343|\n",
      "|        Studio City|       42206.35394275496| 0.6427570883446844|\n",
      "|   West Los Angeles|       40983.06782689424|  0.505868578955071|\n",
      "|      South Carthay|      39642.419795898146|0.47934211829981177|\n",
      "|             Encino|       39546.65508835928| 0.4679921603815911|\n",
      "|       Miracle Mile|       38981.93388699816| 0.5439372236407797|\n",
      "|        Rancho Park|      38740.063860206516|  0.864813343923749|\n",
      "|     Woodland Hills|      38153.839762249285| 0.5016581914658412|\n",
      "+-------------------+------------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "final_LA_df = joined_LA.withColumn(\n",
    "    \"crimes_per_person\",\n",
    "    (col(\"crime_count\") / col(\"total_population\")\n",
    "))\n",
    "final_LA_df.select(\"COMM\", \"median_income_per_person\", \"crimes_per_person\").orderBy(col(\"median_income_per_person\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bd248-9ae7-48ee-8909-79f83c3d33a8",
   "metadata": {},
   "source": [
    "# Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05622b-6de6-426f-9342-24904c2c1340",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d53b0bf-16ef-42ee-a590-5a8d68d6e96d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2842</td><td>application_1732639283265_2801</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2801/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2801_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2728</td><td>application_1732639283265_2687</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2687/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2687_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2729</td><td>application_1732639283265_2688</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2688/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2688_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2736</td><td>application_1732639283265_2695</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2695/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2695_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2738</td><td>application_1732639283265_2697</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2697/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2697_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2741</td><td>application_1732639283265_2700</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2700/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2700_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2743</td><td>application_1732639283265_2702</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2702/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2702_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2753</td><td>application_1732639283265_2712</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2712/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2712_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2754</td><td>application_1732639283265_2713</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2713/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2713_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2762</td><td>application_1732639283265_2721</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2721/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2721_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2767</td><td>application_1732639283265_2726</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2726/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2726_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2773</td><td>application_1732639283265_2732</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2732/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2732_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2776</td><td>application_1732639283265_2735</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2735/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2735_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2779</td><td>application_1732639283265_2738</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2738/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2738_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2780</td><td>application_1732639283265_2739</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2739/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2739_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2783</td><td>application_1732639283265_2742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2742/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2742_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2784</td><td>application_1732639283265_2743</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2743/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2743_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2786</td><td>application_1732639283265_2745</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2745/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2745_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2787</td><td>application_1732639283265_2746</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2746/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2746_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2791</td><td>application_1732639283265_2750</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2750/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2750_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2793</td><td>application_1732639283265_2752</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2752/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2752_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2797</td><td>application_1732639283265_2756</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2756/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2756_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2798</td><td>application_1732639283265_2757</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2757/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2757_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2804</td><td>application_1732639283265_2763</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2763/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2763_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2806</td><td>application_1732639283265_2765</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2765/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2765_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2808</td><td>application_1732639283265_2767</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2767/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2767_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2810</td><td>application_1732639283265_2769</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2769/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2769_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2812</td><td>application_1732639283265_2771</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2771/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2771_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2816</td><td>application_1732639283265_2775</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2775/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2775_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2817</td><td>application_1732639283265_2776</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2776/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2776_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2824</td><td>application_1732639283265_2783</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2783/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2783_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2825</td><td>application_1732639283265_2784</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2784/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2784_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2827</td><td>application_1732639283265_2786</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2786/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2786_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2829</td><td>application_1732639283265_2788</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2788/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2788_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2830</td><td>application_1732639283265_2789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2832</td><td>application_1732639283265_2791</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2791/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2791_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2838</td><td>application_1732639283265_2797</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2797/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2797_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2841</td><td>application_1732639283265_2800</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2800/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2800_01_000006/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2842</td><td>application_1732639283265_2801</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2801/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2801_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "753d8ed6-b63c-4fef-93e4-7459f818ab77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2795</td><td>application_1732639283265_2754</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2754/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2754_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '4g', 'spark.executor.cores': '2'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2704</td><td>application_1732639283265_2663</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2663/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2663_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2719</td><td>application_1732639283265_2678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2678/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2678_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2727</td><td>application_1732639283265_2686</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2686/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2686_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2728</td><td>application_1732639283265_2687</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2687/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2687_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2729</td><td>application_1732639283265_2688</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2688/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2688_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2736</td><td>application_1732639283265_2695</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2695/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2695_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2738</td><td>application_1732639283265_2697</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2697/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2697_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2741</td><td>application_1732639283265_2700</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2700/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2700_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2743</td><td>application_1732639283265_2702</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2702/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2702_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2753</td><td>application_1732639283265_2712</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2712/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2712_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2754</td><td>application_1732639283265_2713</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2713/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2713_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2762</td><td>application_1732639283265_2721</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2721/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2721_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2767</td><td>application_1732639283265_2726</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2726/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2726_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2773</td><td>application_1732639283265_2732</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2732/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2732_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2776</td><td>application_1732639283265_2735</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2735/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2735_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2779</td><td>application_1732639283265_2738</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2738/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2738_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2780</td><td>application_1732639283265_2739</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2739/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2739_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2781</td><td>application_1732639283265_2740</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2740/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2740_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2782</td><td>application_1732639283265_2741</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:8088/cluster/app/application_1732639283265_2741\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:8188/applicationhistory/logs/ip-192-168-1-91.eu-central-1.compute.internal:8041/container_1732639283265_2741_01_000001/container_1732639283265_2741_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2783</td><td>application_1732639283265_2742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2742/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2742_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2784</td><td>application_1732639283265_2743</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2743/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2743_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2785</td><td>application_1732639283265_2744</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2744/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2744_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2786</td><td>application_1732639283265_2745</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2745/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2745_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2787</td><td>application_1732639283265_2746</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2746/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2746_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2791</td><td>application_1732639283265_2750</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2750/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2750_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2793</td><td>application_1732639283265_2752</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2752/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2752_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2795</td><td>application_1732639283265_2754</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2754/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2754_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12ac128e-57d4-40aa-b287-b4dc26410454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2794</td><td>application_1732639283265_2753</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2753/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2753_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2704</td><td>application_1732639283265_2663</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2663/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2663_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2719</td><td>application_1732639283265_2678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2678/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2678_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2727</td><td>application_1732639283265_2686</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2686/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2686_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2728</td><td>application_1732639283265_2687</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2687/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2687_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2729</td><td>application_1732639283265_2688</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2688/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2688_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2736</td><td>application_1732639283265_2695</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2695/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2695_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2738</td><td>application_1732639283265_2697</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2697/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2697_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2741</td><td>application_1732639283265_2700</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2700/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2700_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2743</td><td>application_1732639283265_2702</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2702/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2702_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2753</td><td>application_1732639283265_2712</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2712/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2712_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2754</td><td>application_1732639283265_2713</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2713/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2713_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2762</td><td>application_1732639283265_2721</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2721/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2721_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2767</td><td>application_1732639283265_2726</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2726/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2726_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2773</td><td>application_1732639283265_2732</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2732/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2732_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2776</td><td>application_1732639283265_2735</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2735/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2735_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2779</td><td>application_1732639283265_2738</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2738/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2738_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2780</td><td>application_1732639283265_2739</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2739/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2739_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2781</td><td>application_1732639283265_2740</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2740/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2740_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2782</td><td>application_1732639283265_2741</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:8088/cluster/app/application_1732639283265_2741\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:8188/applicationhistory/logs/ip-192-168-1-91.eu-central-1.compute.internal:8041/container_1732639283265_2741_01_000001/container_1732639283265_2741_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2783</td><td>application_1732639283265_2742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2742/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2742_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2784</td><td>application_1732639283265_2743</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2743/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2743_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2785</td><td>application_1732639283265_2744</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2744/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2744_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2786</td><td>application_1732639283265_2745</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2745/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2745_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2787</td><td>application_1732639283265_2746</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2746/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2746_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2791</td><td>application_1732639283265_2750</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2750/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2750_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2793</td><td>application_1732639283265_2752</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2752/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2752_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2794</td><td>application_1732639283265_2753</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2753/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2753_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad908ee-f2d4-4954-95e4-32120bb0bd77",
   "metadata": {},
   "source": [
    "## PySpark & Sedona imports, Read datasets, register functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f83a591-e948-4c93-9556-26185c544e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BG10: string (nullable = true)\n",
      " |-- BG10FIP10: string (nullable = true)\n",
      " |-- BG12: string (nullable = true)\n",
      " |-- CB10: string (nullable = true)\n",
      " |-- CEN_FIP13: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- CITYCOM: string (nullable = true)\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- CT10: string (nullable = true)\n",
      " |-- CT12: string (nullable = true)\n",
      " |-- CTCB10: string (nullable = true)\n",
      " |-- HD_2012: long (nullable = true)\n",
      " |-- HD_NAME: string (nullable = true)\n",
      " |-- HOUSING10: long (nullable = true)\n",
      " |-- LA_FIP10: string (nullable = true)\n",
      " |-- OBJECTID: long (nullable = true)\n",
      " |-- POP_2010: long (nullable = true)\n",
      " |-- PUMA10: string (nullable = true)\n",
      " |-- SPA_2012: long (nullable = true)\n",
      " |-- SPA_NAME: string (nullable = true)\n",
      " |-- SUP_DIST: string (nullable = true)\n",
      " |-- SUP_LABEL: string (nullable = true)\n",
      " |-- ShapeSTArea: double (nullable = true)\n",
      " |-- ShapeSTLength: double (nullable = true)\n",
      " |-- ZCTA10: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, BooleanType\n",
    "from pyspark.sql.functions import col, udf, sum, avg, regexp_replace, row_number, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flat_census_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "# Print schema\n",
    "flat_census_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d451eba0-d0fe-4383-bd6f-b9bb013f3b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function extract_year at 0x7f62da3f24c0>"
     ]
    }
   ],
   "source": [
    "# UDF - User Defined Functions definitions\n",
    "def extract_year(date_occ: str) -> str:\n",
    "    '''returns year from DATE OCC column'''\n",
    "    return date_occ.split(\"/\")[2].split(\" \")[0]\n",
    "\n",
    "extract_year_udf = udf(extract_year, StringType())\n",
    "# register functions for SQL\n",
    "spark.udf.register(\"extract_year\", extract_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5404963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_2010_19_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "crimes_2020_24_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True)\n",
    "crimes_data = crimes_2010_19_df.union(crimes_2020_24_df)\n",
    "\n",
    "crimes_data_2015 = crimes_2010_19_df \\\n",
    "    .withColumn(\"year\", extract_year_udf(col(\"DATE OCC\"))) \\\n",
    "    .filter(col(\"year\") == \"2015\")\n",
    "\n",
    "census_data = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "RE_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98bad5-5956-43fa-8f0e-56b34aca5110",
   "metadata": {},
   "source": [
    "## Query 4 - DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2e90c8c-fa43-4f36-b12d-bcadc1b0d2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@measure_time\n",
    "def query4_dataframe(debug = False):\n",
    "    '''Shows the two DFs that are the result of the query 4 using DataFrame API'''\n",
    "    global flat_census_data, income_data, crimes_data_2015, RE_data\n",
    "    \n",
    "    \n",
    "    # Filter rows so that dataset refers only to Los Angeles communities\n",
    "    LA_areas = flat_census_data.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "    # Join datasets Census and Average Household Income on Zip Code\n",
    "    joined_df = LA_areas.join(income_data, LA_areas[\"ZCTA10\"] == income_data[\"Zip Code\"], \"inner\")\n",
    "\n",
    "    # Cast the \"Estimated Median Income\" string column as a double\n",
    "    joined_df = joined_df.withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), \"[\\\\$,]\", \"\"))\n",
    "    joined_df = joined_df.withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        col(\"Estimated Median Income\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "    # Group Dataframe by COMM (Community) -  sum of population, households, avg of average income per house, aggregate geometries\n",
    "    weighted_avg = False\n",
    "    if weighted_avg:\n",
    "        LA_comms = joined_df.groupBy(\"COMM\").agg(\n",
    "            sum(\"POP_2010\").alias(\"total_population\"),\n",
    "            sum(\"HOUSING10\").alias(\"total_housing\"),\n",
    "            ( sum(col(\"Estimated Median Income\")*col(\"HOUSING10\"))/ sum(\"HOUSING10\") ).alias(\"average_income_per_house\"),\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"geometry\"))\n",
    "    else:\n",
    "        LA_comms = joined_df.groupBy(\"COMM\").agg(\n",
    "            sum(\"POP_2010\").alias(\"total_population\"),\n",
    "            sum(\"HOUSING10\").alias(\"total_housing\"),\n",
    "            avg(\"Estimated Median Income\").alias(\"average_income_per_house\"),\n",
    "            ST_Union_Aggr(\"geometry\").alias(\"geometry\")) \n",
    "\n",
    "    # Extract median income per person from average income per house, total households and population\n",
    "    result_df = LA_comms.withColumn(\n",
    "        \"median_income_per_person\",\n",
    "        (col(\"total_housing\") * col(\"average_income_per_house\")) / col(\"total_population\")\n",
    "    )\n",
    "\n",
    "    # Order by descending median income per person\n",
    "    sorted_df = result_df.orderBy(col(\"median_income_per_person\").desc())\n",
    "\n",
    "    # Show dataframe\n",
    "    if debug: sorted_df.select(\"COMM\", \"median_income_per_person\").show()\n",
    "\n",
    "    \n",
    "    ############################ Query 4 Starts here - The above was also done in previous query ##################################\n",
    "    \n",
    "    # select the top 3 and the last 3 communities after ranking areas by median_income_per_person\n",
    "    df_ranked = sorted_df.withColumn(\"rank\", row_number().over(Window.orderBy(col(\"median_income_per_person\").desc())))\n",
    "    top_3 = df_ranked.filter(df_ranked.rank <= 3).select(\"COMM\", \"median_income_per_person\", \"geometry\")\n",
    "    bottom_3 = df_ranked.filter(df_ranked.rank > (df_ranked.count() - 3)).select(\"COMM\", \"median_income_per_person\", \"geometry\")\n",
    "    if debug: top_3.show()\n",
    "    if debug: bottom_3.show()\n",
    "\n",
    "    # append geom column in crime data based on LON, LAT\n",
    "    crime_geo = crimes_data_2015.withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "    # crime_geo_RE JOIN (the above top_3 and bottom_3 dataframes) on ST_Within(crime_geo.geom, result_df.geometry) ...\n",
    "    crimes_top_3_comm = top_3 \\\n",
    "        .join(crime_geo, ST_Within(crime_geo.geom, top_3.geometry), \"inner\")\n",
    "    crimes_bottom_3_comm = bottom_3 \\\n",
    "        .join(crime_geo, ST_Within(crime_geo.geom, bottom_3.geometry), \"inner\")\n",
    "\n",
    "    # results above JOIN with RE codes on Vict Descent (same on both datasets)\n",
    "    crimes_top_3_comm_RE = crimes_top_3_comm \\\n",
    "        .join(RE_data, RE_data[\"Vict Descent\"] == crimes_top_3_comm[\"Vict Descent\"], \"inner\") \\\n",
    "        .select( col(\"DR_NO\"), col(\"AREA NAME\"), col(\"LON\"), col(\"LAT\"), col(\"geom\"), col(\"Vict Descent Full\") \\\n",
    "        )\n",
    "    crimes_bottom_3_comm_RE = crimes_bottom_3_comm \\\n",
    "        .join(RE_data, RE_data[\"Vict Descent\"] == crimes_bottom_3_comm[\"Vict Descent\"], \"inner\") \\\n",
    "        .select( col(\"DR_NO\"), col(\"AREA NAME\"), col(\"LON\"), col(\"LAT\"), col(\"geom\"), col(\"Vict Descent Full\") \\\n",
    "        )\n",
    "\n",
    "    # group by Vict Descent select Vict Descent Full, count(*) as #\n",
    "    crimes_top_3_comm_grouped = crimes_top_3_comm_RE \\\n",
    "        .groupBy(\"Vict Descent Full\") \\\n",
    "        .agg(count(\"*\").alias(\"#\"))\n",
    "    crimes_top_3_comm_grouped = crimes_top_3_comm_grouped.select(\"Vict Descent Full\", \"#\")\n",
    "    crimes_bottom_3_comm_grouped = crimes_bottom_3_comm_RE \\\n",
    "        .groupBy(\"Vict Descent Full\") \\\n",
    "        .agg(count(\"*\").alias(\"#\")) \n",
    "    crimes_bottom_3_comm_grouped = crimes_bottom_3_comm_grouped.select(\"Vict Descent Full\", \"#\")\n",
    "\n",
    "    # order by # descending\n",
    "    crimes_top_3_comm_grouped = crimes_top_3_comm_grouped.orderBy(\"#\", ascending=False)\n",
    "    crimes_bottom_3_comm_grouped = crimes_bottom_3_comm_grouped.orderBy(\"#\", ascending=False)\n",
    "\n",
    "    # show\n",
    "    crimes_top_3_comm_grouped.show()\n",
    "    crimes_bottom_3_comm_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368228b1-9bf2-42a0-9145-6d6169ab36d8",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d15648c7-4117-4242-a0c5-3f149593207d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 2g\n",
      "Executor Cores: 1\n",
      "+--------------------+---+\n",
      "|   Vict Descent Full|  #|\n",
      "+--------------------+---+\n",
      "|               White|894|\n",
      "|               Other|156|\n",
      "|Hispanic/Latin/Me...| 97|\n",
      "|               Black| 55|\n",
      "|             Unknown| 50|\n",
      "|         Other Asian| 31|\n",
      "|American Indian/A...|  1|\n",
      "|             Chinese|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|   Vict Descent Full|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3191|\n",
      "|               Black| 872|\n",
      "|               White| 430|\n",
      "|               Other| 265|\n",
      "|         Other Asian| 140|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  24|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken by query4_dataframe: 73.36 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query4_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9aa19aec-886e-44e4-be22-713c7c84d44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 4g\n",
      "Executor Cores: 2\n",
      "+--------------------+---+\n",
      "|   Vict Descent Full|  #|\n",
      "+--------------------+---+\n",
      "|               White|894|\n",
      "|               Other|156|\n",
      "|Hispanic/Latin/Me...| 97|\n",
      "|               Black| 55|\n",
      "|             Unknown| 50|\n",
      "|         Other Asian| 31|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|   Vict Descent Full|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3191|\n",
      "|               Black| 872|\n",
      "|               White| 430|\n",
      "|               Other| 265|\n",
      "|         Other Asian| 140|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  24|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken by query4_dataframe: 68.58 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query4_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab96fb99-44f3-4d69-804e-76740557030f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 8g\n",
      "Executor Cores: 4\n",
      "+--------------------+---+\n",
      "|   Vict Descent Full|  #|\n",
      "+--------------------+---+\n",
      "|               White|894|\n",
      "|               Other|156|\n",
      "|Hispanic/Latin/Me...| 97|\n",
      "|               Black| 55|\n",
      "|             Unknown| 50|\n",
      "|         Other Asian| 31|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|   Vict Descent Full|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3191|\n",
      "|               Black| 872|\n",
      "|               White| 430|\n",
      "|               Other| 265|\n",
      "|         Other Asian| 140|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  24|\n",
      "|              Korean|   5|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken by query4_dataframe: 57.63 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query4_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd04246-c4da-4512-a646-6ab5dc3d96b3",
   "metadata": {},
   "source": [
    "# Query 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25826414-f106-45f6-aeec-b4060a5bc0c5",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6dd77b-7520-4935-9429-58636d7d8a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1268</td><td>application_1732639283265_1231</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1231/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1231_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1208</td><td>application_1732639283265_1171</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1171/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1171_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1211</td><td>application_1732639283265_1174</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1174/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1174_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1212</td><td>application_1732639283265_1175</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1175/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1175_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1213</td><td>application_1732639283265_1176</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1176/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1176_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1233</td><td>application_1732639283265_1196</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1196/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1196_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1236</td><td>application_1732639283265_1199</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1199/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1199_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1238</td><td>application_1732639283265_1201</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1201/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1201_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1240</td><td>application_1732639283265_1203</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1203/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1203_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1243</td><td>application_1732639283265_1206</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1206/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1206_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1244</td><td>application_1732639283265_1207</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1207/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1207_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1246</td><td>application_1732639283265_1209</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1209/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1209_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1248</td><td>application_1732639283265_1211</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1211/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1211_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1251</td><td>application_1732639283265_1214</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1214/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1214_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1253</td><td>application_1732639283265_1216</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1216/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1216_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1255</td><td>application_1732639283265_1218</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1218/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1218_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1256</td><td>application_1732639283265_1219</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1219/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1219_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1260</td><td>application_1732639283265_1223</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1223/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1223_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1265</td><td>application_1732639283265_1228</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1228/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1228_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1266</td><td>application_1732639283265_1229</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1229/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1229_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1268</td><td>application_1732639283265_1231</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1231/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1231_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e1af32-d66c-41be-82ba-f1de33c9bb48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1269</td><td>application_1732639283265_1232</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1232/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1232_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '4g', 'spark.executor.cores': '2'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1208</td><td>application_1732639283265_1171</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1171/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1171_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1211</td><td>application_1732639283265_1174</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1174/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1174_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1212</td><td>application_1732639283265_1175</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1175/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1175_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1213</td><td>application_1732639283265_1176</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1176/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1176_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1233</td><td>application_1732639283265_1196</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1196/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1196_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1236</td><td>application_1732639283265_1199</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1199/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1199_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1238</td><td>application_1732639283265_1201</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1201/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1201_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1240</td><td>application_1732639283265_1203</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1203/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1203_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1243</td><td>application_1732639283265_1206</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1206/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1206_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1244</td><td>application_1732639283265_1207</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1207/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1207_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1246</td><td>application_1732639283265_1209</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1209/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1209_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1248</td><td>application_1732639283265_1211</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1211/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1211_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1251</td><td>application_1732639283265_1214</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1214/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1214_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1253</td><td>application_1732639283265_1216</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1216/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1216_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1255</td><td>application_1732639283265_1218</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1218/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1218_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1256</td><td>application_1732639283265_1219</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1219/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1219_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1260</td><td>application_1732639283265_1223</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1223/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1223_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1265</td><td>application_1732639283265_1228</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1228/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1228_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1266</td><td>application_1732639283265_1229</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1229/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1229_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1269</td><td>application_1732639283265_1232</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1232/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1232_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a42783f-f5e9-474f-9971-b8d2c72ebc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1452</td><td>application_1732639283265_1413</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1413/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1413_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '8', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1451</td><td>application_1732639283265_1412</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1412/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1412_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1452</td><td>application_1732639283265_1413</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1413/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1413_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16f8bf-30eb-4014-bcaf-633ee2d5fdfd",
   "metadata": {},
   "source": [
    "## PySpark imports, Read datasets, register functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a568ba62-e0ea-4a3a-9062-0f19b784586e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, BooleanType\n",
    "from pyspark.sql.functions import col, udf, sum, max, min, avg, count, mean, when, monotonically_increasing_id, dense_rank, window, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# read datasets\n",
    "crimes_2010_19_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "crimes_2020_24_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True)\n",
    "crimes_df = crimes_2010_19_df.union(crimes_2020_24_df)\n",
    "police_stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c98284-8ba9-4bc6-a61e-67b07951d524",
   "metadata": {},
   "source": [
    "## Query 5 - DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7d2958-943b-4279-baa3-06a9dfe49db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@measure_time\n",
    "def query5_dataframe(df, debug = False):\n",
    "    '''Shows the DF that is result of the query 5 using DataFrame API'''\n",
    "    global police_stations_df\n",
    "    \n",
    "    joined_df = df.select('DR_NO', 'AREA NAME', 'LON', 'LAT') \\\n",
    "        .filter( (col('LON') != '0') & (col('LON') != 0) ) \\\n",
    "        .withColumn(\"crime_point\", ST_Point(\"LON\", \"LAT\")) \\\n",
    "        .join(police_stations_df) \\\n",
    "        .withColumn(\"police_point\", ST_Point(\"X\", \"Y\")) \\\n",
    "        .withColumn('distance', ST_DistanceSphere(\"crime_point\", \"police_point\")/1000) # divide with 1000 to conver into km \\\n",
    "\n",
    "    crimes_in_null_island = df.filter( (col('LON') == '0') | (col('LON') == 0) ).count()\n",
    "        \n",
    "    if debug: joined_df.filter(col('DR_NO') == '001307355').show(30)\n",
    "    # In this DF, for each DR_NO we have 21 rows for the distances between the crime location and police departments \n",
    "\n",
    "    # Define a window \n",
    "    windowSpec = Window.partitionBy(\"DR_NO\")\n",
    "    extended_df = joined_df.withColumn(\"min_distance\", when(col(\"distance\").isNotNull(), min(\"distance\").over(windowSpec)).otherwise(None)) \\\n",
    "        .filter(col('distance') == col('min_distance'))\n",
    "    if debug: extended_df.filter(col('DR_NO') == '001307355').show(30)\n",
    "    if debug: extended_df.filter(col('DIVISION') == 'HOLLENBECK').orderBy('min_distance', ascending=True).show(30)\n",
    "    if debug: extended_df.filter(col('DIVISION') == 'HOLLENBECK').agg(avg('min_distance').alias('avg')).show()\n",
    "    # Now we have each DR_NO only once\n",
    "\n",
    "    grouped_df = extended_df.groupBy(\"DIVISION\") \\\n",
    "        .agg( \\\n",
    "             avg(\"min_distance\").alias(\"average_distance\"), \\\n",
    "             count(\"*\").alias(\"#\") \\\n",
    "        ) \\\n",
    "        .select('DIVISION', 'average_distance', '#') \\\n",
    "        .orderBy([\"#\"], ascending=[False])\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"DIVISION\", StringType(), nullable=True), \n",
    "        StructField(\"average_distance\", FloatType(), nullable=True), \n",
    "        StructField(\"#\", IntegerType(), nullable=False) \n",
    "    ])\n",
    "    # Create a DataFrame for the Null Island row\n",
    "    null_island_row = spark.createDataFrame([\n",
    "        (\"Unknown\", None, crimes_in_null_island) \n",
    "    ], schema)\n",
    "\n",
    "    grouped_df = grouped_df.union(null_island_row)\n",
    "    \n",
    "    if debug: grouped_df.agg(sum('#').alias('incidents')).show()\n",
    "\n",
    "    # This DF is grouped by police departments/divisions \n",
    "    # so we see the average distance of crimes that happened closer to that and the number of these incidents/crimes\n",
    "    grouped_df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07935f7c-e05c-4c78-b740-c39f538c132e",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f2c44b-5869-4799-beb6-aff8b29a8eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 2\n",
      "Executor Memory: 8g\n",
      "Executor Cores: 4\n",
      "+----------------+------------------+------+\n",
      "|        DIVISION|  average_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787205|224340|\n",
      "|        VAN NUYS| 2.953369742819787|210134|\n",
      "|       SOUTHWEST|2.1913988057808838|188901|\n",
      "|        WILSHIRE|2.5926655329787796|185996|\n",
      "|     77TH STREET| 1.716544971970102|171827|\n",
      "|         OLYMPIC|1.7236036971780937|170897|\n",
      "| NORTH HOLLYWOOD|2.6430060941415676|167854|\n",
      "|         PACIFIC|3.8500706553079027|161359|\n",
      "|         CENTRAL|0.9924764374568903|153871|\n",
      "|         RAMPART|1.5345341879190049|152736|\n",
      "|       SOUTHEAST|2.4218662158881794|152176|\n",
      "|     WEST VALLEY| 3.035671216314078|138643|\n",
      "|         TOPANGA|3.2969548417555608|138217|\n",
      "|        FOOTHILL| 4.250921708424991|134896|\n",
      "|          HARBOR|3.7025615993565033|126747|\n",
      "|      HOLLENBECK|2.6801812377068224|115837|\n",
      "|WEST LOS ANGELES| 2.792457289034108|115781|\n",
      "|          NEWTON|1.6346357397097435|111110|\n",
      "|       NORTHEAST|3.6236655246040765|108109|\n",
      "|         MISSION| 3.690942614278606|103355|\n",
      "|      DEVONSHIRE|2.8247654128008244| 77094|\n",
      "|         Unknown|              NULL|  3457|\n",
      "+----------------+------------------+------+\n",
      "\n",
      "Time taken by query5_dataframe: 27.66 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query5_dataframe(crimes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d36a4c1-1ace-48c5-9d34-fe25c266cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 4\n",
      "Executor Memory: 4g\n",
      "Executor Cores: 2\n",
      "+----------------+------------------+------+\n",
      "|        DIVISION|  average_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787196|224340|\n",
      "|        VAN NUYS|2.9533697428197883|210134|\n",
      "|       SOUTHWEST| 2.191398805780884|188901|\n",
      "|        WILSHIRE|2.5926655329787796|185996|\n",
      "|     77TH STREET|1.7165449719701025|171827|\n",
      "|         OLYMPIC|1.7236036971780941|170897|\n",
      "| NORTH HOLLYWOOD| 2.643006094141567|167854|\n",
      "|         PACIFIC|3.8500706553079027|161359|\n",
      "|         CENTRAL|0.9924764374568901|153871|\n",
      "|         RAMPART|1.5345341879190044|152736|\n",
      "|       SOUTHEAST| 2.421866215888179|152176|\n",
      "|     WEST VALLEY|3.0356712163140793|138643|\n",
      "|         TOPANGA|3.2969548417555603|138217|\n",
      "|        FOOTHILL| 4.250921708424989|134896|\n",
      "|          HARBOR| 3.702561599356503|126747|\n",
      "|      HOLLENBECK|2.6801812377068237|115837|\n",
      "|WEST LOS ANGELES|2.7924572890341084|115781|\n",
      "|          NEWTON|1.6346357397097424|111110|\n",
      "|       NORTHEAST|3.6236655246040756|108109|\n",
      "|         MISSION|3.6909426142786046|103355|\n",
      "|      DEVONSHIRE|2.8247654128008253| 77094|\n",
      "|         Unknown|              NULL|  3457|\n",
      "+----------------+------------------+------+\n",
      "\n",
      "Time taken by query5_dataframe: 24.56 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query5_dataframe(crimes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7817b190-88b4-4266-9b41-a243acd53cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 8\n",
      "Executor Memory: 2g\n",
      "Executor Cores: 1\n",
      "+----------------+------------------+------+\n",
      "|        DIVISION|  average_distance|     #|\n",
      "+----------------+------------------+------+\n",
      "|       HOLLYWOOD|2.0762639601787205|224340|\n",
      "|        VAN NUYS|2.9533697428197865|210134|\n",
      "|       SOUTHWEST|2.1913988057808846|188901|\n",
      "|        WILSHIRE|2.5926655329787787|185996|\n",
      "|     77TH STREET|1.7165449719701025|171827|\n",
      "|         OLYMPIC|1.7236036971780935|170897|\n",
      "| NORTH HOLLYWOOD|2.6430060941415676|167854|\n",
      "|         PACIFIC|3.8500706553079027|161359|\n",
      "|         CENTRAL|0.9924764374568898|153871|\n",
      "|         RAMPART|1.5345341879190046|152736|\n",
      "|       SOUTHEAST| 2.421866215888179|152176|\n",
      "|     WEST VALLEY|3.0356712163140793|138643|\n",
      "|         TOPANGA|3.2969548417555603|138217|\n",
      "|        FOOTHILL| 4.250921708424989|134896|\n",
      "|          HARBOR|3.7025615993565038|126747|\n",
      "|      HOLLENBECK|2.6801812377068233|115837|\n",
      "|WEST LOS ANGELES|2.7924572890341075|115781|\n",
      "|          NEWTON| 1.634635739709743|111110|\n",
      "|       NORTHEAST|3.6236655246040756|108109|\n",
      "|         MISSION| 3.690942614278606|103355|\n",
      "|      DEVONSHIRE|2.8247654128008235| 77094|\n",
      "|         Unknown|              NULL|  3457|\n",
      "+----------------+------------------+------+\n",
      "\n",
      "Time taken by query5_dataframe: 44.33 seconds"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n",
    "query5_dataframe(crimes_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
